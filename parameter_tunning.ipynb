{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6532b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48852ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "import json \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-7s %(message)s',\n",
    "                    stream=sys.stderr, level=logging.INFO)\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.INFO)\n",
    "\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "\n",
    "#General ML \n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, silhouette_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2,f_classif\n",
    "from modules.clustering_helpers import select_labeled_samples\n",
    "#In-house Module Imports\n",
    "from config import Configuration \n",
    "from datasets import EclipseSampledDataset, VoltaSampledDataset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from utils import *\n",
    "import hdbscan\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31760ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "### new ML models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da48a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings\n",
    "user = \"aksar\"\n",
    "logging.warning(f'Are you sure that you are: {user}?')\n",
    "OUTPUT_DIR = f'/projectnb/peaclab-mon/{user}/active_learning_experiments'\n",
    "classifier_name = 'rf'\n",
    "num_samples_per_pair = 1\n",
    "NUM_FEATURE  = 2000\n",
    "SYSTEM = 'volta'\n",
    "FE_NAME = 'tsfresh'\n",
    "EXP_NAME  = 'tsfresh_experiments'\n",
    "CV_INDEX = 0\n",
    "FS_NAME = \"CHI\"\n",
    "FEATURE_SELECTION = False\n",
    "SCALER = 'None' #For now, do the scaling inside the notebook, then you can move that to the class function\n",
    "MODEL_CONFIG = 'tuning_results' #rf_tuncer or rf_tuncer_worst_case\n",
    "logging.warning('Results will be generated in {}, double check please!'.format(MODEL_CONFIG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Configuration(ipython=True,\n",
    "                     overrides={\n",
    "                         'output_dir': Path(OUTPUT_DIR), #change\n",
    "                         'system' : SYSTEM,\n",
    "                         'exp_name':EXP_NAME,                                                  \n",
    "                         'cv_fold':CV_INDEX, \n",
    "                         'model_config': MODEL_CONFIG,\n",
    "                     })\n",
    "\n",
    "with open(str(conf['experiment_dir']) + '/anom_dict.json') as f:\n",
    "    ANOM_DICT = json.load(f)\n",
    "with open(str(conf['experiment_dir']) + '/app_dict.json') as f:\n",
    "    APP_DICT = json.load(f) \n",
    "    \n",
    "APP_REVERSE_DICT = {}\n",
    "for app_name, app_encoding in APP_DICT.items():\n",
    "    APP_REVERSE_DICT[app_encoding] = app_name    \n",
    "\n",
    "ANOM_REVERSE_DICT = {}\n",
    "for anom_name, anom_encoding in ANOM_DICT.items():\n",
    "    ANOM_REVERSE_DICT[anom_encoding] = anom_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SYSTEM == \"eclipse\":\n",
    "        eclipseDataset = EclipseSampledDataset(conf)\n",
    "        train_data, train_label, test_data, test_label = eclipseDataset.load_dataset(\n",
    "            cv_fold=CV_INDEX,\n",
    "            scaler=SCALER,\n",
    "            borghesi=False,\n",
    "            mvts=True if FE_NAME == \"mvts\" else False,\n",
    "            tsfresh=True if FE_NAME == \"tsfresh\" else False,\n",
    "        )\n",
    "\n",
    "elif SYSTEM == \"volta\":\n",
    "    voltaDataset = VoltaSampledDataset(conf)\n",
    "    train_data, train_label, test_data, test_label = voltaDataset.load_dataset(\n",
    "        cv_fold=CV_INDEX,\n",
    "        scaler=SCALER,\n",
    "        borghesi=False,\n",
    "        mvts=True if FE_NAME == \"mvts\" else False,\n",
    "        tsfresh=True if FE_NAME == \"tsfresh\" else False,\n",
    "    )\n",
    "\n",
    "assert list(train_data.index) == list(train_label.index)  # check the order of the labels\n",
    "assert list(test_data.index) == list(test_label.index)  # check the order of the labels\n",
    "\n",
    "if FEATURE_SELECTION:\n",
    "    selected_features = pd.read_csv(conf[\"experiment_dir\"] / \"selected_features.csv\")\n",
    "    train_data = train_data[list(selected_features[\"0\"].values)]\n",
    "    test_data = test_data[list(selected_features[\"0\"].values)]\n",
    "\n",
    "train_label[\"anom_names\"] = train_label.apply(lambda x: ANOM_REVERSE_DICT[x[\"anom\"]], axis=1)\n",
    "train_label[\"app_names\"] = train_label[\"app\"].apply(lambda x: APP_REVERSE_DICT[x])\n",
    "test_label[\"anom_names\"] = test_label.apply(lambda x: ANOM_REVERSE_DICT[x[\"anom\"]], axis=1)\n",
    "test_label[\"app_names\"] = test_label[\"app\"].apply(lambda x: APP_REVERSE_DICT[x])\n",
    "\n",
    "all_data = pd.concat([train_data, test_data])\n",
    "all_data = all_data.dropna(axis=1, how=\"any\")\n",
    "all_label = pd.concat([train_label, test_label])\n",
    "\n",
    "train_data = all_data.loc[train_label.index]\n",
    "test_data = all_data.loc[test_label.index]\n",
    "\n",
    "logging.info(\"Train data shape %s\", train_data.shape)\n",
    "logging.info(\"Train label shape %s\", train_label.shape)\n",
    "logging.info(\"Test data shape %s\", test_data.shape)\n",
    "logging.info(\"Test label shape %s\", test_label.shape)\n",
    "\n",
    "logging.info(\"Train data label dist: \\n%s\", train_label[\"anom\"].value_counts())\n",
    "logging.info(\"Test data label dist: \\n%s\", test_label[\"anom\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = \"MinMax\"\n",
    "\n",
    "if SCALER == \"MinMax\":\n",
    "\n",
    "    minmax_scaler = MinMaxScaler().fit(train_data)\n",
    "    train_data = pd.DataFrame(\n",
    "        minmax_scaler.transform(train_data), columns=train_data.columns, index=train_data.index\n",
    "    )\n",
    "    test_data = pd.DataFrame(\n",
    "        minmax_scaler.transform(test_data), columns=test_data.columns, index=test_data.index\n",
    "    )\n",
    "\n",
    "elif SCALER == \"Standard\":\n",
    "\n",
    "    # Standardize data (per feature Z-normalization, i.e. zero-mean and unit variance)\n",
    "    scaler = StandardScaler().fit(train_data)\n",
    "    train_data = pd.DataFrame(\n",
    "        scaler.transform(train_data), columns=train_data.columns, index=train_data.index\n",
    "    )\n",
    "    test_data = pd.DataFrame(\n",
    "        scaler.transform(test_data), columns=test_data.columns, index=test_data.index\n",
    "    )\n",
    "\n",
    "# Implement new feature selection strategies below\n",
    "if FS_NAME == \"CHI\":\n",
    "\n",
    "    selector = SelectKBest(chi2, k=NUM_FEATURE)\n",
    "    selector.fit(train_data, train_label[\"anom\"])\n",
    "    train_data = train_data[train_data.columns[selector.get_support(indices=True)]]\n",
    "    selected_columns = train_data.columns\n",
    "    test_data = test_data[test_data.columns & selected_columns]\n",
    "\n",
    "elif FS_NAME == \"TSFRESH\":\n",
    "    logging.warning(\n",
    "        \"NUM_FEATURE parameter will be overwritten by the automatic selection process\"\n",
    "    )\n",
    "\n",
    "    y_train = train_label[\"anom\"]\n",
    "    X_train = train_data\n",
    "\n",
    "    relevant_features = set()\n",
    "\n",
    "    for label in y_train.unique():\n",
    "        y_train_binary = y_train == label\n",
    "        X_train_filtered = tsfresh.select_features(X_train, y_train_binary)\n",
    "        print(\n",
    "            \"Number of relevant features for class {}: {}/{}\".format(\n",
    "                label, X_train_filtered.shape[1], X_train.shape[1]\n",
    "            )\n",
    "        )\n",
    "        relevant_features = relevant_features.union(set(X_train_filtered.columns))\n",
    "    train_data = train_data[relevant_features]\n",
    "    test_data = test_data[relevant_features]\n",
    "    NUM_FEATURE = len(relevant_features)\n",
    "\n",
    "elif FS_NAME == \"NONE\":\n",
    "    logging.info(\"No feature selection strategy is specified, will be using all features\")\n",
    "    NUM_FEATURE = len(train_data.columns)\n",
    "\n",
    "logging.info(train_data.shape)\n",
    "logging.info(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_label = pd.read_csv(\n",
    "    conf[\"experiment_dir\"]\n",
    "    / f\"CV_{CV_INDEX}\"\n",
    "    / f\"labeled_train_label_{num_samples_per_pair}.csv\",\n",
    "    index_col=[\"node_id\"],\n",
    ")\n",
    "labeled_test_label = pd.read_csv(\n",
    "    conf[\"experiment_dir\"]\n",
    "    / f\"CV_{CV_INDEX}\"\n",
    "    / f\"labeled_test_label_{num_samples_per_pair}.csv\",\n",
    "    index_col=[\"node_id\"],\n",
    ")\n",
    "node_indices_labeled = list(labeled_train_label[\"anom\"].index.values)\n",
    "\n",
    "logging.info(\"Labeled data label dist: \\n%s\", labeled_train_label[\"anom\"].value_counts())\n",
    "logging.info(\"Unlabeled data label dist: \\n%s\", labeled_test_label[\"anom\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a new column for label status\n",
    "node_indices_unlabeled = []\n",
    "for node in train_label.index:\n",
    "    if node not in node_indices_labeled:\n",
    "        node_indices_unlabeled.append(node)\n",
    "train_label[\"label_status\"] = train_label[\"anom\"]  # for the full data case\n",
    "train_label[\"label_status\"] = np.where(\n",
    "    train_label.index.get_level_values(\"node_id\").isin(node_indices_unlabeled),\n",
    "    -1,\n",
    "    train_label[\"label_status\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_labeled_pool contains one sample from each application anomaly pair\n",
    "initial_labeled_pool = train_label[(train_label[\"label_status\"] != -1)]\n",
    "# Active learning or random sampling will be querying from the same pool\n",
    "initial_unlabeled_pool = train_label[(train_label[\"label_status\"] == -1)]\n",
    "\n",
    "if classifier_name == \"rf\":\n",
    "    selected_classifier = RandomForestClassifier()\n",
    "elif classifier_name == \"lr\":\n",
    "    selected_classifier = LogisticRegression()\n",
    "else:\n",
    "    selected_classifier = RandomForestClassifier()\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "all_app_names = list(APP_DICT.keys())\n",
    "selected_apps = dict.fromkeys(all_app_names, 0)\n",
    "selected_anoms = dict.fromkeys(list(ANOM_REVERSE_DICT.keys()), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052aec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = { \n",
    "            'n_estimators': [100, 200, 500, 1000, 2000, 10000],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'max_depth' : [4,8,12,None],\n",
    "            'criterion' :['gini', 'entropy']\n",
    "        }\n",
    "\n",
    "lgbm_param_grid = {\n",
    "            \"num_leaves\": [2, 8, 31, 128],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "            \"max_depth\": [-1, 2, 8],\n",
    "            \"colsample_bytree\": [0.5, 1.0],\n",
    "        }\n",
    "\n",
    "lr_param_grid = {'penalty' : ['l1', 'l2'],\n",
    "                'C' : [0.1, 0.5, 1.0, 3.0, 5.0],\n",
    "                'solver' : ['liblinear']}\n",
    "\n",
    "mlp_param_grid = {\n",
    "            \"max_iter\": [100, 200, 500, 1000],\n",
    "            \"hidden_layer_sizes\": [(10, 10, 10), (30, 20, 10), (50, 100, 50), (100)],\n",
    "            \"alpha\": [0.0001, 0.001, 0.01],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_single_param_grid = { \n",
    "            'n_estimators': [10,50,100,200],\n",
    "            'max_depth' : [None,2,4,8,10,20],\n",
    "            'criterion' :['gini','entropy']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f551e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_single_param_grid_trial = { \n",
    "            'n_estimators': [10],\n",
    "            'max_depth' : [50],\n",
    "            'criterion' :['gini']\n",
    "        }\n",
    "\n",
    "lr_single_param_grid = { \n",
    "            'penalty': ['l1','l2'],\n",
    "             'C' : [0.1],\n",
    "            'solver' : ['liblinear']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "models = ['random_forest', 'logistic_regression', 'lgbm','mlp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f828e34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for model in models:\n",
    "    if model == 'random_forest':\n",
    "        logging.info(\"Tunning Random Forest...\")\n",
    "        rfc = RandomForestClassifier(random_state=42)\n",
    "        rfc.fit(train_data, train_label['anom'])  # previously we were giving x_initial, y_initial\n",
    "        pred = rfc.predict(test_data)\n",
    "        report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "        print(\"Inıtial Macro-Avg  F-1 for Random Forest on Test data: \",report_dict[\"macro avg\"][\"f1-score\"])\n",
    "        CV_rfc = GridSearchCV(estimator=rfc, param_grid=rf_param_grid, cv= 5)\n",
    "        CV_rfc.fit(train_data, train_label['anom'])\n",
    "        logging.info(CV_rfc.best_params_)\n",
    "        best_max_features = CV_rfc.best_params_['max_features']\n",
    "        best_n_estimators = CV_rfc.best_params_['n_estimators']\n",
    "        best_max_depth    = CV_rfc.best_params_['max_depth']\n",
    "        best_criterion    = CV_rfc.best_params_['criterion']\n",
    "        tuned_rfc=RandomForestClassifier(random_state=42, max_features= best_max_features, n_estimators= best_n_estimators, max_depth=best_max_depth, criterion=best_criterion)\n",
    "        tuned_rfc.fit(train_data, train_label['anom'])\n",
    "        pred = tuned_rfc.predict(test_data)\n",
    "        report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "        print(\"Tuned Macro-Avg  F-1 for Random Forest on Test data: \",report_dict[\"macro avg\"][\"f1-score\"])\n",
    "        \n",
    "    elif model == 'lgbm':\n",
    "        logging.info(\"Tunning LGBM...\")\n",
    "        train_data = train_data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "        lgbm = LGBMClassifier(objective='multiclass', random_state=5)\n",
    "        lgbm.fit(x_initial, y_initial)\n",
    "        pred = lgbm.predict(test_data)\n",
    "        report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "        print(\"Inıtial Macro-Avg  F-1 for LGBM on Test data: \",report_dict[\"macro avg\"][\"f1-score\"])\n",
    "        CV_lgbm = GridSearchCV(estimator=lgbm, param_grid= lgbm_param_grid, cv= 5)\n",
    "        CV_lgbm.fit(x_initial, y_initial)\n",
    "        logging.info(CV_lgbm.best_params_)\n",
    "        best_n_estimators  = CV_lgbm.best_params_['n_estimators']\n",
    "        best_max_depth     = CV_lgbm.best_params_['max_depth']\n",
    "        best_learning_rate = CV_lgbm.best_params_['learning_rate']\n",
    "        best_lambda_l1     = CV_lgbm.best_params_['best_lambda_l1']\n",
    "        best_lambda_l2     = CV_lgbm.best_params_['best_lambda_l2']\n",
    "        tuned_lgbm = LGBMClassifier(random_state = 5, n_estimators= best_n_estimators, max_depth=best_max_depth, learning_rate = best_learning_rate, lambda_l1 = best_lambda_l1, lambda_l2 = best_lambda_l2 )\n",
    "        tuned_lgbm.fit(x_initial, y_initial)\n",
    "        pred = tuned_lgbm.predict(test_data)\n",
    "        report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "        print(\"Tuned Macro-Avg  F-1 for Random Forest on Test data: \",report_dict[\"macro avg\"][\"f1-score\"])\n",
    "        \n",
    "    elif model == 'logistic_regression':\n",
    "        logging.info(\"Tunning Logistic Regression...\")\n",
    "        lr = LogisticRegression(random_state=0, dual=False, max_iter=12000)\n",
    "        lr.fit(x_initial, y_initial)\n",
    "        pred = lr.predict(test_data)\n",
    "        report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "        print(\"Inıtial Macro-Avg  F-1 forLogistic Regression on Test data: \",report_dict[\"macro avg\"][\"f1-score\"])\n",
    "        CV_lr = GridSearchCV(estimator = lr, param_grid=lr_param_grid, cv= 5)\n",
    "        CV_lr.fit(x_initial, y_initial)\n",
    "        logging.info(CV_lr.best_params_)\n",
    "        best_penalty = CV_lr.best_params_['penalty']\n",
    "        best_C = CV_lr.best_params_['C']        \n",
    "        best_solver = CV_lr.best_params_['solver']\n",
    "        tuned_lr = LogisticRegression(max_iter=12000, dual=False, random_state=0, penalty= best_penalty, C = best_C, solver=best_solver)\n",
    "        tuned_lr.fit(x_initial, y_initial)\n",
    "        pred = tuned_lr.predict(test_data)\n",
    "        report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "        print(\"Tuned Macro-Avg  F-1 for Logistic Regression Test data: \",report_dict[\"macro avg\"][\"f1-score\"])\n",
    "        \n",
    "    elif model == \"mlp\":\n",
    "        logging.info(\"Tunning MLP...\")\n",
    "        mlp = MLPClassifier(random_state=1, max_iter=300).fit(x_initial, y_initial)\n",
    "        pred = mlp.predict(test_data)\n",
    "        report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "        print(\"Tuned Macro-Avg  F-1 for MLP Regression Test data: \",report_dict[\"macro avg\"][\"f1-score\"])\n",
    "        CV_mlp = GridSearchCV(estimator = mlp, param_grid=mlp_param_grid, cv= 5)\n",
    "        CV_mlp.fit(x_initial, y_initial)\n",
    "        logging.info(CV_mlp.best_params_)\n",
    "        \n",
    "        best_max_iter           = CV_mlp.best_params_['max_iter']     \n",
    "        best_hidden_layer_sizes = CV_mlp.best_params_['hidden_layer_sizes']\n",
    "        best_alpha              = CV_mlp.best_params_['alpha']\n",
    "        \n",
    "        tuned_mlp = MLPClassifier(max_iter=best_max_iter,hidden_layer_sizes = best_hidden_layer_sizes, alpha = best_alpha)\n",
    "        tuned_mlp.fit(x_initial, y_initial)\n",
    "        pred = tuned_mlp.predict(test_data)\n",
    "        report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "        print(\"Tuned Macro-Avg  F-1 for MLP Test data: \",report_dict[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logging.info(\"Tunning Random Forest...\")\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(train_data, train_label['anom'])  # previously we were giving x_initial, y_initial\n",
    "pred = rfc.predict(test_data)\n",
    "initial_report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\"Inıtial Macro-Avg  F-1 for Random Forest on Test data: \",initial_report_dict[\"macro avg\"][\"f1-score\"])\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=rf_single_param_grid, cv= 5, scoring = 'f1_macro')\n",
    "CV_rfc.fit(train_data, train_label['anom'])\n",
    "logging.info(CV_rfc.best_params_)\n",
    "pred = CV_rfc.predict(test_data)\n",
    "final_report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\"Tuned Macro-Avg  F-1 for Random Forest on Test data: \",final_report_dict[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logging.info(f\"Tunning {MODEL}...\")\n",
    "clf.fit(train_data, train_label[\"anom\"])  # previously we were giving x_initial, y_initial\n",
    "pred = clf.predict(test_data)\n",
    "initial_report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\n",
    "    f\"Inıtial Macro-Avg  F-1 for {MODEL} on Test data: \",\n",
    "    initial_report_dict[\"macro avg\"][\"f1-score\"],\n",
    ")\n",
    "CV_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring=\"f1_macro\")\n",
    "CV_clf.fit(train_data, train_label[\"anom\"])\n",
    "logging.info(CV_clf.best_params_)\n",
    "pred = CV_clf.predict(test_data)\n",
    "final_report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\n",
    "    f\"Tuned Macro-Avg  F-1 for {MODEL} on Test data: \",\n",
    "    final_report_dict[\"macro avg\"][\"f1-score\"],\n",
    ")\n",
    "CV_clf.best_params_[\"initial_f1_score\"] = initial_report_dict[\"macro avg\"][\"f1-score\"]\n",
    "CV_clf.best_params_[\"tuned_f1_score\"] = final_report_dict[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "jsonpath = conf[\"results_dir\"] / f\"{MODEL}_Best_Params.json\"\n",
    "jsonpath.write_text(json.dumps(CV_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc.best_params_['initial_f1_score'] = initial_report_dict[\"macro avg\"][\"f1-score\"]\n",
    "CV_rfc.best_params_['tuned_f1_score'] = final_report_dict[\"macro avg\"][\"f1-score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec02976",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc.best_params_['tuned_f1_score'] = report_dict[\"macro avg\"][\"f1-score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9bacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logging.info(\"Tunning Random Forest...\")\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(train_data, train_label['anom'])  # previously we were giving x_initial, y_initial\n",
    "pred = rfc.predict(test_data)\n",
    "report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\"Inıtial Macro-Avg  F-1 for Random Forest on Test data: \",report_dict[\"macro avg\"][\"f1-score\"])\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=rf_single_param_grid_trial, cv= 5, scoring = 'f1_macro')\n",
    "CV_rfc.fit(train_data, train_label['anom'])\n",
    "logging.info(CV_rfc.best_params_)\n",
    "pred = CV_rfc.predict(test_data)\n",
    "report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\"Tuned Macro-Avg  F-1 for Random Forest on Test data: \",report_dict[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d898596",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonpath = conf['results_dir'] / 'RandomForest_Params.json'\n",
    "jsonpath.write_text(json.dumps(CV_rfc.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logging.info(\"Tunning Logistic Regression...\")\n",
    "lr = LogisticRegression(random_state=0, dual=False, max_iter=12000)\n",
    "lr.fit(train_data, train_label['anom'])  # previously we were giving x_initial, y_initial\n",
    "pred = lr.predict(test_data)\n",
    "report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\"Inıtial Macro-Avg  F-1 for LR on Test data: \",report_dict[\"macro avg\"][\"f1-score\"])\n",
    "CV_lr = GridSearchCV(estimator=lr, param_grid=lr_single_param_grid, cv= 5, scoring = 'f1_macro')\n",
    "CV_lr.fit(train_data, train_label['anom'])\n",
    "logging.info(CV_lr.best_params_)\n",
    "pred = CV_lr.predict(test_data)\n",
    "report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\"Tuned Macro-Avg  F-1 for Random Forest on Test data: \",report_dict[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_mlp = MLPClassifier(max_iter=1000,hidden_layer_sizes = 100, alpha = 0.0001)\n",
    "tuned_mlp.fit(x_initial, y_initial)\n",
    "pred = tuned_mlp.predict(test_data)\n",
    "report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\"Tuned Macro-Avg  F-1 for MLP Test data: \",report_dict[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier(objective='multiclass')\n",
    "#renamed_train_data = train_data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "#renamed_train_data = renamed_train_data.to_string(header=False)\n",
    "lgbm.fit(train_data.values,train_label['anom'])\n",
    "y_pred = lgbm.predict(test_data)\n",
    "report_dict = classification_report(test_label[\"anom\"], y_pred, output_dict=True)\n",
    "print(\"Inıtial Macro-Avg  F-1 for LGBM Test data: \",report_dict[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4cabec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MODEL = 'lgbm'\n",
    "if MODEL == \"random_forest\":\n",
    "        param_grid = {\n",
    "            \"n_estimators\": [8, 10, 20, 100, 200],\n",
    "            \"max_depth\": [None, 4, 8, 10, 20],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "        }\n",
    "\n",
    "        clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "elif MODEL == \"logistic_regression\":\n",
    "\n",
    "    param_grid = {\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        \"solver\": [\"liblinear\"],\n",
    "    }\n",
    "\n",
    "    clf = LogisticRegression(random_state=0, dual=False, max_iter=12000)\n",
    "\n",
    "elif MODEL == \"mlp\":\n",
    "\n",
    "    param_grid = {\n",
    "        \"max_iter\": [100, 200, 500, 1000],\n",
    "        \"hidden_layer_sizes\": [(10, 10, 10), (30, 20, 10), (50, 100, 50), (100)],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    }\n",
    "\n",
    "    clf = MLPClassifier(random_state=1)\n",
    "\n",
    "elif MODEL == \"lgbm\":\n",
    "\n",
    "    param_grid = {\n",
    "        \"num_leaves\": [2, 8, 31, 128],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "        \"max_depth\": [-1, 2, 8],\n",
    "        \"colsample_bytree\": [0.5, 1.0],\n",
    "    }\n",
    "    clf = LGBMClassifier(objective=\"multiclass\", random_state=5)\n",
    "    train_data = train_data.values\n",
    "\n",
    "else:\n",
    "    raise (\"Invalid classifier\")\n",
    "\n",
    "logging.info(f\"Tunning {MODEL}...\")\n",
    "clf.fit(train_data, train_label[\"anom\"])  # previously we were giving x_initial, y_initial\n",
    "pred = clf.predict(test_data)\n",
    "initial_report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\n",
    "    f\"Inıtial Macro-Avg  F-1 for {MODEL} on Test data: \",\n",
    "    initial_report_dict[\"macro avg\"][\"f1-score\"],\n",
    ")\n",
    "CV_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring=\"f1_macro\")\n",
    "CV_clf.fit(train_data, train_label[\"anom\"])\n",
    "logging.info(CV_clf.best_params_)\n",
    "pred = CV_clf.predict(test_data)\n",
    "final_report_dict = classification_report(test_label[\"anom\"], pred, output_dict=True)\n",
    "print(\n",
    "    f\"Tuned Macro-Avg  F-1 for {MODEL} on Test data: \",\n",
    "    final_report_dict[\"macro avg\"][\"f1-score\"],\n",
    ")\n",
    "CV_clf.best_params_[\"initial_f1_score\"] = initial_report_dict[\"macro avg\"][\"f1-score\"]\n",
    "CV_clf.best_params_[\"tuned_f1_score\"] = final_report_dict[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "jsonpath = conf[\"results_dir\"] / f\"{MODEL}_Best_Params.json\"\n",
    "jsonpath.write_text(json.dumps(CV_clf.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
