{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a3f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77679f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e502be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sns.set_context('paper')\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-7s %(message)s',\n",
    "                    stream=sys.stderr, level=logging.INFO)\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.INFO)\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "#In-house Module Imports\n",
    "from config import Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf084e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_query_line_plot(stats_df,param_dict,naming_dict, color_dict, use_std=True):\n",
    "    \n",
    "    strategies = stats_df['query_strategy'].unique()\n",
    "\n",
    "    fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "\n",
    "    for strategy in strategies:\n",
    "        selected_score_data = stats_df.loc[stats_df['query_strategy'] == strategy]\n",
    "        x = selected_score_data['query_iter']\n",
    "        y = selected_score_data['mean']\n",
    "        plt.plot(x, \n",
    "                 y,\n",
    "                 linewidth=3,\n",
    "                 color=color_dict[strategy],\n",
    "                 label=strategy)\n",
    "        if use_std:\n",
    "            plt.fill_between(x, \n",
    "                             y - (0.5*selected_score_data['std']), \n",
    "                             y + (0.5*selected_score_data['std']),\n",
    "                             alpha=0.2,  \n",
    "                             color=color_dict[strategy],\n",
    "                            )\n",
    "        else:\n",
    "            \n",
    "            plt.fill_between(x, \n",
    "                 selected_score_data['ci95_lo'], \n",
    "                 selected_score_data['ci95_hi'],\n",
    "                 alpha=1, \n",
    "                 color=color_dict[strategy],\n",
    "                )\n",
    "\n",
    "    legend = plt.legend(title=\"\",prop={'size': param_dict['legend_size']})\n",
    "    \n",
    "    plt.title(f\"{naming_dict['title_prefix']} w.r.t Number of Queries \\n {naming_dict['fe_name']} - {naming_dict['system']} - {naming_dict['num_features']} \\n #Known Apps in Training: {naming_dict['num_train_known_apps']}\",fontsize=param_dict['title_size'])\n",
    "    #plt.ylim([0.3, 1.01])\n",
    "    plt.xlim([0, 50])\n",
    "\n",
    "    plt.ylabel(f\"{naming_dict['title_prefix']}\",size=param_dict['y_label_font'])\n",
    "    plt.xlabel(\"Number of Iterations\",size=param_dict['x_label_font'])\n",
    "    plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "    plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3815a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(df, score_type):\n",
    "    \n",
    "    stats = df.groupby(['query_strategy','query_iter'])[score_type].agg(['mean', 'count', 'std'])\n",
    "\n",
    "    ci95_hi = []\n",
    "    ci95_lo = []\n",
    "    z_star = 1.96 # 1.65: ci90\n",
    "\n",
    "    for i in stats.index:\n",
    "        m, c, s = stats.loc[i]#[2:5]\n",
    "        ci95_hi.append(m + z_star*s/math.sqrt(c))\n",
    "        ci95_lo.append(m - z_star*s/math.sqrt(c))\n",
    "\n",
    "    stats['ci95_hi'] = ci95_hi\n",
    "    stats['ci95_lo'] = ci95_lo\n",
    "\n",
    "    stats.reset_index(['query_strategy','query_iter'], inplace=True)    \n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40751f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_DIR = 'active_learning_experiments'\n",
    "SYSTEM = 'volta'\n",
    "FE_NAME = 'tsfresh'\n",
    "EXP_NAME = f'{FE_NAME}_experiments'\n",
    "DIR_NAME_TO_GENERATE_RESULTS = 'exp_2_active_learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad48259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f'/projectnb/peaclab-mon/{user}/{PARENT_DIR}' # or feature_extraction_experiments\n",
    "CV_INDEX = 0\n",
    "SCALER = 'None' #For now, do the scaling inside the notebook, then you can move that to the class function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Configuration(ipython=True,\n",
    "                     overrides={\n",
    "                         'output_dir': Path(OUTPUT_DIR), #change\n",
    "                         'system' : SYSTEM,\n",
    "                         'exp_name':EXP_NAME,                                                  \n",
    "                         'cv_fold':CV_INDEX, \n",
    "                         'model_config': DIR_NAME_TO_GENERATE_RESULTS\n",
    "                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762edadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "counter = 0\n",
    "result_list = []\n",
    "\n",
    "for cv_index in [0,1,2,3,4]:\n",
    "\n",
    "    conf = Configuration(ipython=True,\n",
    "                         overrides={\n",
    "                             'output_dir': Path(OUTPUT_DIR), #change\n",
    "                             'system' : SYSTEM,\n",
    "                             'exp_name':EXP_NAME,                                                  \n",
    "                             'cv_fold':cv_index, \n",
    "                             'model_config': DIR_NAME_TO_GENERATE_RESULTS\n",
    "                         }\n",
    "                        )\n",
    "\n",
    "    for filename in os.listdir(conf['results_dir']):\n",
    "        \n",
    "        only_filename = filename.split('.')\n",
    "        splitted_filename = only_filename[0].split('#')\n",
    "        train_apps = splitted_filename[0].split(':')[1]\n",
    "        test_apps = splitted_filename[1].split(':')[1]\n",
    "        num_unknown_test_apps = len(test_apps.split(\"-\"))\n",
    "        num_known_train_apps = len(train_apps.split(\"-\"))        \n",
    "        \n",
    "        temp_csv = pd.read_csv(Path(conf['results_dir']) / filename,index_col = [0])\n",
    "        temp_csv['train_apps'] = train_apps\n",
    "        temp_csv['test_apps'] = test_apps        \n",
    "        temp_csv['num_unknown_test_apps'] = num_unknown_test_apps\n",
    "        temp_csv['num_known_train_apps'] = num_known_train_apps\n",
    "        temp_csv['repeat_num'] = int(splitted_filename[8])\n",
    "        \n",
    "        result_list.append(temp_csv)\n",
    "        \n",
    "        counter += 1\n",
    "               \n",
    "    result_df = pd.concat(result_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_query_strategies = result_df['query_strategy'].unique()\n",
    "logging.info(\"Unique query strategies: %s\",unique_query_strategies)\n",
    "unique_methods = result_df['method'].unique()\n",
    "logging.info(\"Unique Methods: %s\", unique_methods)\n",
    "unique_fe_methods = result_df['fe'].unique()\n",
    "logging.info(\"Feature Extraction Methods: %s\",unique_fe_methods)\n",
    "unique_feature_counts = sorted(result_df['feature_count'].unique())\n",
    "logging.info(\"Num Features: %s\", unique_feature_counts)\n",
    "unique_query_sizes = sorted(result_df['query_size'].unique())\n",
    "logging.info(\"Unique query sizes: %s\",unique_query_sizes)\n",
    "unique_known_train_apps = result_df['num_known_train_apps'].unique()\n",
    "logging.info(\"Number of Known Apps in the Training: %s\", unique_known_train_apps)\n",
    "\n",
    "# unique_random_selection = len(result_df[result_df['query_strategy'] == 'random']['repeat_num'].unique())\n",
    "# logging.info(\"Number of Repeats for the Random Selection: %s\", unique_random_selection)\n",
    "\n",
    "# unique_folds = len(result_df[result_df['query_strategy'] != 'random']['fold'].unique())\n",
    "# logging.info(\"Number of Folds for the Active Learning Methods: %s\", unique_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeca912",
   "metadata": {},
   "source": [
    "## Paper Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc8ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "                'entropy': 'tab:purple',\n",
    "                'margin': 'tab:blue',    \n",
    "                'uncertainty': 'orange',    \n",
    "                'random': 'tab:green',\n",
    "                'equal_app' : 'tab:red',\n",
    "             }\n",
    "\n",
    "param_dict = {\n",
    "                'fig_width': 48,\n",
    "                'fig_height': 12,\n",
    "                'y_label_font': 40,\n",
    "                'x_label_font': 40,\n",
    "                'x_ticks_font': 35,\n",
    "                'y_ticks_font': 35,    \n",
    "                'legend_size': 40,\n",
    "                'legend_title_size': 40,\n",
    "                'title_size': 45,\n",
    "                'title_pad': 40, \n",
    "                'fig_title_size': 55,\n",
    "    \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df0c2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'RandomForestClassifier'\n",
    "num_query = 250\n",
    "feature_count = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f37213af",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict = {}\n",
    "naming_dict['fe_name'] = FE_NAME\n",
    "naming_dict['system'] = SYSTEM\n",
    "naming_dict['num_features'] = int(feature_count)\n",
    "naming_dict['num_queries'] = num_query  \n",
    "naming_dict['model'] = selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8386132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_result_df = result_df[(result_df['query_size'] == num_query) & (result_df['model'] == selected_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c038c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_2_result_df = selected_result_df[selected_result_df['num_known_train_apps'] == 2]\n",
    "known_4_result_df = selected_result_df[selected_result_df['num_known_train_apps'] == 4]\n",
    "known_6_result_df = selected_result_df[selected_result_df['num_known_train_apps'] == 6]\n",
    "\n",
    "# stats_far = calculate_stats(selected_result_df, 'false_alarm_rate')\n",
    "# stats_amr = calculate_stats(selected_result_df, 'anomaly_miss_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "676fe55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_2_stats_fscore = calculate_stats(known_2_result_df, 'macro_avg_f1_score')\n",
    "known_4_stats_fscore = calculate_stats(known_4_result_df, 'macro_avg_f1_score')\n",
    "known_6_stats_fscore = calculate_stats(known_6_result_df, 'macro_avg_f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_query_line_plot(fscore_df, far_df, amr_df, param_dict, naming_dict, color_dict, num_query=50, use_std=False):\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(1,3,figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "    \n",
    "    for ind, (stats_df,fig_title, fig_y_label) in enumerate(zip([fscore_df,far_df,amr_df],[\"2 Known Apps\",\"4 Known Apps\",\"6 Known Apps\"],[\"F1-score (Macro Avg)\",\"\",\"\"])):\n",
    "        \n",
    "        strategies = stats_df['query_strategy'].unique()\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            \n",
    "            selected_score_data = stats_df.loc[stats_df['query_strategy'] == strategy]\n",
    "            x = selected_score_data['query_iter']\n",
    "            y = selected_score_data['mean']\n",
    "            axs[ind].plot(x, \n",
    "                     y,\n",
    "                     linewidth=3,\n",
    "                     color=color_dict[strategy],\n",
    "                     label=strategy)\n",
    "            if use_std:\n",
    "                axs[ind].fill_between(x, \n",
    "                                 y - (1*selected_score_data['std']), \n",
    "                                 y + (1*selected_score_data['std']),\n",
    "                                 alpha=0.2,  \n",
    "                                 color=color_dict[strategy],\n",
    "                                )\n",
    "            else:\n",
    "\n",
    "                axs[ind].fill_between(x, \n",
    "                     selected_score_data['ci95_lo'], \n",
    "                     selected_score_data['ci95_hi'],\n",
    "                     alpha=0.1, \n",
    "                     color=color_dict[strategy],\n",
    "                    )\n",
    "        axs[ind].set_title(fig_title, fontsize=param_dict['title_size'])     \n",
    "        axs[ind].set_xlim([0, num_query])\n",
    "        \n",
    "        axs[ind].set_ylabel(f\"{fig_y_label}\",size=param_dict['y_label_font'])\n",
    "        axs[ind].set_xlabel(\"Number of Iterations\",size=param_dict['x_label_font'])\n",
    "        axs[ind].tick_params(axis='x', labelsize=param_dict['x_ticks_font'])\n",
    "        axs[ind].tick_params(axis='y', labelsize=param_dict['y_ticks_font'])                \n",
    "    \n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, [\"Random\",\"Uncertainty\"], loc='lower left', \n",
    "               bbox_to_anchor=(0.38, 1.0, 0.3, 0.4), ncol=5, frameon=True, mode='None',\n",
    "               prop={'size': param_dict['legend_size']}\n",
    "              )\n",
    "        \n",
    "    #fig.suptitle(f\"Active Learning with {naming_dict['model']}\",fontsize=param_dict['fig_title_size'])        \n",
    "    fig.suptitle(f\"Unseen Applications - {naming_dict['system'].capitalize()}\",fontsize=param_dict['fig_title_size'], y = 1.20) \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"/usr3/graduate/baksar/projectx/AI4HPCAnalytics/src/active_learning_experiments/plots/{naming_dict['system']}_{naming_dict['fe_name']}_unseen_apps.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_query_line_plot(known_2_stats_fscore, known_4_stats_fscore, known_6_stats_fscore, \n",
    "                              param_dict, naming_dict, color_dict, \n",
    "                              num_query=num_query,\n",
    "                              use_std=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0402c3",
   "metadata": {},
   "source": [
    "## Plot for Each Feature Extraction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4c5fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_known_apps = 8 \n",
    "feature_count = unique_feature_counts[0]\n",
    "temp_result_df = result_df[result_df['num_known_train_apps'] == num_train_known_apps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b63d5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_fscore = calculate_stats(temp_result_df, 'macro_avg_f1_score')\n",
    "stats_far = calculate_stats(temp_result_df, 'false_alarm_rate')\n",
    "stats_amr = calculate_stats(temp_result_df, 'anomaly_miss_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faec892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict = {}\n",
    "naming_dict['fe_name'] = FE_NAME\n",
    "naming_dict['system'] = SYSTEM\n",
    "naming_dict['num_features'] = int(feature_count)\n",
    "naming_dict['num_queries'] = unique_query_sizes[0]\n",
    "naming_dict['num_train_known_apps'] = int(num_train_known_apps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36f4646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "                'entropy': 'tab:purple',\n",
    "                'margin': 'tab:blue',    \n",
    "                'uncertainty': 'orange',    \n",
    "                'random': 'tab:green',\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "569dda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "                'fig_width': 18,\n",
    "                'fig_height': 12,\n",
    "                'y_label_font': 45,\n",
    "                'x_label_font': 45,\n",
    "                 'x_ticks_font': 42,\n",
    "                 'y_ticks_font': 50,    \n",
    "                'legend_size': 30,\n",
    "                'legend_title_size': 40,\n",
    "                'title_size': 36,\n",
    "                'title_pad': 40,             \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9586c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict['title_prefix'] = \"F1-Score (Macro Avg)\"\n",
    "plot_query_line_plot(stats_fscore, param_dict, naming_dict, color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict['title_prefix'] = \"False Alarm Rate\"\n",
    "plot_query_line_plot(stats_far, param_dict, naming_dict, color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict['title_prefix'] = \"Anomaly Miss Rate\"\n",
    "plot_query_line_plot(stats_amr, param_dict, naming_dict, color_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
