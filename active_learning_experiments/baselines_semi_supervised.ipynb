{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a97189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464cab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "554860c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "import json \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-7s %(message)s',\n",
    "                    stream=sys.stderr, level=logging.INFO)\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "#General ML \n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, silhouette_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "#In-house Module Imports\n",
    "from config import Configuration \n",
    "from datasets import EclipseSampledDataset, VoltaSampledDataset\n",
    "from utils import *\n",
    "\n",
    "from modules.models import BaseAutoencoder\n",
    "from modules.models_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaf2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readModelConfig(conf, exp_name,cv_index,model_name,system):\n",
    "    \"\"\"Reads saved config file and returns as a dictionary\"\"\"\n",
    "    \n",
    "    import math    \n",
    "    #config_path = Path('/projectnb/peaclab-mon/aksar/clustering_experiments/{system}/{exp_name}/CV_{cv_index}/{model_name}/model_config.csv'.format(system=system,exp_name=exp_name,cv_index=cv_index,model_name=model_name))\n",
    "    config_path = conf['model_config_dir'] / \"model_config.csv\" #/ f\"{model_name}/model_config.csv\" \n",
    "    print(config_path)\n",
    "    \n",
    "    conf = {}\n",
    "    try:\n",
    "        conf_csv = pd.read_csv(config_path)\n",
    "    except:\n",
    "        logging.info(\"Config.csv doesn't exist\")\n",
    "    \n",
    "\n",
    "    for column in conf_csv.columns:\n",
    "        if isinstance(conf_csv[column][0],str):\n",
    "            if 'dir' in column:\n",
    "                conf[column] = Path(conf_csv[column][0])\n",
    "            else:\n",
    "                conf[column] = conf_csv[column][0]\n",
    "                \n",
    "        #FIXME: Find a generic comparison for integers\n",
    "        elif isinstance(conf_csv[column][0],np.int64):\n",
    "                conf[column] = conf_csv[column][0]  \n",
    "                \n",
    "        elif isinstance(conf_csv[column][0],np.bool_):\n",
    "                conf[column] = conf_csv[column][0]                  \n",
    "        else:\n",
    "            if math.isnan(conf_csv[column][0]):\n",
    "                conf[column] = None\n",
    "        \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e3c1cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, params):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.params = params\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "          #layers.Dropout(params['dropout']),                        \n",
    "          #layers.Dense(1000, activation=params['hidden_acts']),\n",
    "          layers.Dense(params['latent_dim'], activation=params['latent_activation'],name='code')\n",
    "        ],name='Encoder')\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "          #layers.Dense(1000, activation=params['hidden_acts']),                        \n",
    "          #layers.Dropout(params['dropout']),                        \n",
    "          layers.Dense(x_dim, activation='linear'),            \n",
    "        ],name='Decoder')\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"model_name\": self.params['model_name'],\n",
    "            #Layer related stuff\n",
    "            \"latent_dim\": self.params['latent_dim'],\n",
    "            \"latent_activation\": self.params['latent_activation'],\n",
    "            \"dropout_rate\": self.params['dropout'],\n",
    "            \"regularizer\": self.params['regularizer'],            \n",
    "            \"regularization_rate\": self.params['regularization_rate'],                        \n",
    "            #Compilation related stuff            \n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"loss\": self.loss,\n",
    "            #Training\n",
    "            \"epochs\": self.params['epochs']\n",
    "        }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3ecf5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = \"proctor_exp_1_active_learning\"  # change this\n",
    "SYSTEM = 'volta'  # volta or eclipse\n",
    "FE_NAME = 'tsfresh' #tsfresh, or mvts => It will set the EXP_NAME. Be careful. \n",
    "NUM_FEATURE = 250  # example: 250 ,2000, 4000\n",
    "classifier_name = 'proctor'\n",
    "query_strategy = \"random\"  # \"uncertainty\", \"margin\", \"entropy\", \"random\", \"equal_app\"\n",
    "CV_INDEX = 0  # it can be integer value within the range 0 1 2 3 4\n",
    "repeat_num = 0\n",
    "query_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fb66b9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 05:15:04,047 WARNING Results will be generated in proctor_exp_1_active_learning, double check please!\n"
     ]
    }
   ],
   "source": [
    "#Constants\n",
    "FS_NAME = \"CHI\"\n",
    "method = \"baseline\" if (query_strategy == 'random' or query_strategy == 'equal_app')  else \"active_learning\"\n",
    "num_samples_per_pair = 1\n",
    "\n",
    "if SYSTEM == 'volta':\n",
    "    OUTPUT_DIR = f'/projectnb/peaclab-mon/{user}/active_learning_experiments'\n",
    "elif SYSTEM == 'eclipse':    \n",
    "    OUTPUT_DIR = f'/projectnb/peaclab-mon/{user}/active_learning_experiments_final_hdfs'\n",
    "    \n",
    "EXP_NAME = f'{FE_NAME}_experiments'\n",
    "FEATURE_SELECTION = False\n",
    "SCALER = 'None' #For now, do the scaling inside the notebook, then you can move that to the class function\n",
    "\n",
    "logging.warning('Results will be generated in {}, double check please!'.format(MODEL_CONFIG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  conf = Configuration(ipython=True,\n",
    "                     overrides={\n",
    "                         'output_dir': Path(OUTPUT_DIR), #change\n",
    "                         'system' : SYSTEM,\n",
    "                         'exp_name':EXP_NAME,                                                  \n",
    "                         'cv_fold':CV_INDEX, \n",
    "                         'model_config': MODEL_CONFIG,\n",
    "                     })\n",
    "\n",
    "with open(str(conf['experiment_dir']) + '/anom_dict.json') as f:\n",
    "    ANOM_DICT = json.load(f)\n",
    "with open(str(conf['experiment_dir']) + '/app_dict.json') as f:\n",
    "    APP_DICT = json.load(f)    \n",
    "    \n",
    "APP_REVERSE_DICT = {}\n",
    "for app_name, app_encoding in APP_DICT.items():\n",
    "    APP_REVERSE_DICT[app_encoding] = app_name    \n",
    "\n",
    "ANOM_REVERSE_DICT = {}\n",
    "for anom_name, anom_encoding in ANOM_DICT.items():\n",
    "    ANOM_REVERSE_DICT[anom_encoding] = anom_name      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210dca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SYSTEM == 'eclipse':\n",
    "    eclipseDataset = EclipseSampledDataset(conf)\n",
    "    train_data, train_label, test_data, test_label = eclipseDataset.load_dataset(scaler=SCALER,\n",
    "                                                                                 cv_fold=CV_INDEX,\n",
    "                                                                                 borghesi=False, \n",
    "                                                                                 mvts=True if FE_NAME == 'mvts' else False, \n",
    "                                                                                 tsfresh=True if FE_NAME == 'tsfresh' else False)\n",
    "\n",
    "elif SYSTEM == 'volta':\n",
    "    voltaDataset = VoltaSampledDataset(conf)\n",
    "    train_data, train_label, test_data, test_label = voltaDataset.load_dataset(scaler=SCALER,\n",
    "                                                                               cv_fold=CV_INDEX,\n",
    "                                                                               borghesi=False,\n",
    "                                                                               mvts=True if FE_NAME == 'mvts' else False,\n",
    "                                                                               tsfresh=True if FE_NAME == 'tsfresh' else False)\n",
    "\n",
    "assert list(train_data.index) == list(train_label.index) #check the order of the labels     \n",
    "assert list(test_data.index) == list(test_label.index) #check the order of the labels    \n",
    "\n",
    "if FEATURE_SELECTION:\n",
    "    selected_features = pd.read_csv(conf['experiment_dir'] / 'selected_features.csv')\n",
    "    train_data = train_data[list(selected_features['0'].values)]\n",
    "    test_data = test_data[list(selected_features['0'].values)]\n",
    "    \n",
    "train_label['anom_names'] = train_label.apply(lambda x: ANOM_REVERSE_DICT[x['anom']], axis=1)\n",
    "train_label['app_names']=train_label['app'].apply(lambda x: APP_REVERSE_DICT[x])\n",
    "test_label['anom_names'] = test_label.apply(lambda x: ANOM_REVERSE_DICT[x['anom']], axis=1)\n",
    "test_label['app_names']=test_label['app'].apply(lambda x: APP_REVERSE_DICT[x])\n",
    "\n",
    "all_data = pd.concat([train_data, test_data])\n",
    "all_data = all_data.dropna(axis=1, how='any')\n",
    "all_label = pd.concat([train_label,test_label])\n",
    "\n",
    "train_data = all_data.loc[train_label.index]\n",
    "test_data = all_data.loc[test_label.index]\n",
    "    \n",
    "logging.info(\"Train data shape %s\",train_data.shape)\n",
    "logging.info(\"Train label shape %s\",train_label.shape)\n",
    "logging.info(\"Test data shape %s\",test_data.shape)  \n",
    "logging.info(\"Test label shape %s\",test_label.shape)\n",
    "\n",
    "logging.info(\"Train data label dist: \\n%s\",train_label['anom'].value_counts())\n",
    "logging.info(\"Test data label dist: \\n%s\",test_label['anom'].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ca776e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_data = train_data.copy()\n",
    "orig_test_data = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8f8c0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = orig_train_data\n",
    "# test_data = orig_test_data\n",
    "# print(len(train_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efe9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = 'MinMax'\n",
    "\n",
    "if SCALER == 'MinMax':\n",
    "    \n",
    "    minmax_scaler = MinMaxScaler().fit(train_data)\n",
    "    train_data = pd.DataFrame(minmax_scaler.transform(train_data),columns=train_data.columns,index=train_data.index)\n",
    "    test_data = pd.DataFrame(minmax_scaler.transform(test_data),columns=test_data.columns,index=test_data.index)\n",
    "    \n",
    "elif SCALER == 'Standard':\n",
    "    \n",
    "    # Standardize data (per feature Z-normalization, i.e. zero-mean and unit variance)        \n",
    "    scaler = StandardScaler().fit(train_data)\n",
    "    train_data = pd.DataFrame(scaler.transform(train_data),columns=train_data.columns,index=train_data.index)\n",
    "    test_data = pd.DataFrame(scaler.transform(test_data),columns=test_data.columns,index=test_data.index)  \n",
    "    \n",
    "#Implement new feature selection strategies below\n",
    "if FS_NAME == 'CHI':\n",
    "    \n",
    "    selector = SelectKBest(chi2, k=NUM_FEATURE)\n",
    "    selector.fit(train_data,train_label['anom'])\n",
    "    train_data = train_data[train_data.columns[selector.get_support(indices=True)]]\n",
    "    selected_columns = train_data.columns\n",
    "    test_data = test_data[test_data.columns & selected_columns]\n",
    "    \n",
    "elif FS_NAME == 'TSFRESH':\n",
    "    logging.warning(\"NUM_FEATURE parameter will be overwritten by the automatic selection process\")\n",
    "    \n",
    "    y_train = train_label['anom']\n",
    "    X_train = train_data\n",
    "\n",
    "    relevant_features = set()\n",
    "\n",
    "    for label in y_train.unique():\n",
    "        y_train_binary = y_train == label\n",
    "        X_train_filtered = tsfresh.select_features(X_train, y_train_binary)\n",
    "        print(\"Number of relevant features for class {}: {}/{}\".format(label, X_train_filtered.shape[1], X_train.shape[1]))\n",
    "        relevant_features = relevant_features.union(set(X_train_filtered.columns))    \n",
    "    train_data = train_data[relevant_features]\n",
    "    test_data = test_data[relevant_features]\n",
    "    NUM_FEATURE = len(relevant_features)\n",
    "    \n",
    "elif FS_NAME == 'NONE':\n",
    "    logging.info(\"No feature selection strategy is specified, will be using all features\")\n",
    "    NUM_FEATURE = len(train_data.columns)\n",
    "    \n",
    "logging.info(train_data.shape)\n",
    "logging.info(test_data.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c342e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the node_ids considered labeled\n",
    "labeled_train_label = pd.read_csv(conf['experiment_dir'] / f'CV_{CV_INDEX}'/ f'labeled_train_label_{num_samples_per_pair}.csv', index_col=['node_id'])\n",
    "labeled_test_label = pd.read_csv(conf['experiment_dir'] / f'CV_{CV_INDEX}'/ f'labeled_test_label_{num_samples_per_pair}.csv', index_col=['node_id'])\n",
    "node_indices_labeled = list(labeled_train_label['anom'].index.values)\n",
    "\n",
    "logging.info(\"Labeled data label dist: \\n%s\",labeled_train_label['anom'].value_counts())\n",
    "logging.info(\"Unlabeled data label dist: \\n%s\",labeled_test_label['anom'].value_counts())\n",
    "\n",
    "#Set a new column for label status\n",
    "node_indices_unlabeled = []\n",
    "for node in train_label.index:\n",
    "    if node not in node_indices_labeled:\n",
    "        node_indices_unlabeled.append(node)\n",
    "        \n",
    "train_label['label_status'] = train_label['anom'] # for the full data case\n",
    "train_label['label_status'] = np.where(train_label.index.get_level_values('node_id').isin(node_indices_unlabeled), -1,train_label['label_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "415d0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_labeled_pool contains one sample from each application anomaly pair\n",
    "initial_labeled_pool = train_label[(train_label['label_status'] != -1)]\n",
    "\n",
    "#Active learning or random sampling will be querying from the same pool\n",
    "initial_unlabeled_pool = train_label[(train_label['label_status'] == -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b6b0eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "#Compile related\n",
    "\n",
    "params['loss'] = 'mse'\n",
    "\n",
    "params['optimizer'] = 'adadelta'\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "#Layer related\n",
    "params['dropout'] = 0 #This will add dropout after layers\n",
    "params['regularizer'] = None\n",
    "params['regularization_rate'] = None\n",
    "\n",
    "params['hidden_layers'] = []\n",
    "params['hidden_acts'] = 'relu'\n",
    "\n",
    "params['latent_dim'] =  2000\n",
    "params['latent_activation'] = 'relu' #change the activation of the code layer\n",
    "\n",
    "#Training related\n",
    "params['epochs'] = 10\n",
    "\n",
    "if params['optimizer'] == 'adam':\n",
    "    opt = optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "elif params['optimizer'] == 'adadelta':\n",
    "    opt = optimizers.Adadelta(learning_rate=params['learning_rate'])\n",
    "elif params['optimizer'] == 'sgd':\n",
    "    opt = optimizers.SGD(learning_rate=params['learning_rate'])\n",
    "\n",
    "\n",
    "if params['loss'] == 'mse':\n",
    "    selected_loss = losses.MeanSquaredError()\n",
    "elif params['loss'] == 'mae':\n",
    "    selected_loss = losses.MeanAbsoluteError()\n",
    "    \n",
    "params['model_name'] = f\"dae_{params['latent_dim']}_{params['epochs']}_{params['latent_activation']}\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f23f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Train data shape %s\",train_data.shape)\n",
    "logging.info(\"Train label shape %s\",train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILD AE\n",
    "\n",
    "TRAIN_DATA = train_data.values\n",
    "\n",
    "x_dim = TRAIN_DATA.shape[1]\n",
    "\n",
    "autoencoder = Autoencoder(params)       \n",
    "\n",
    "autoencoder.compile(optimizer=opt, loss=selected_loss)\n",
    "autoencoder.build(TRAIN_DATA.shape) \n",
    "autoencoder.encoder.summary()\n",
    "autoencoder.decoder.summary()\n",
    "autoencoder.get_config()    \n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', \n",
    "                                    verbose=1,\n",
    "                                    patience=10,\n",
    "                                    mode='min',\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "history = autoencoder.fit(train_data.values, train_data.values,\n",
    "                epochs=params['epochs'],\n",
    "                shuffle=True,\n",
    "                callbacks =[\n",
    "                              es  \n",
    "                            ],\n",
    "                validation_split = 0.2,\n",
    "                )\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "if not history.history[\"val_loss\"] is None:\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "autoencoder.save(str(conf['model_dir'] / (params['model_name'])))\n",
    "logging.info(\"Model saved!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9abba14",
   "metadata": {},
   "source": [
    "### Train the supervised part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "81737760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/peaclab-mon/aksar/active_learning_experiments/volta/tsfresh_experiments/CV_0/proctor_exp_1_active_learning/model_config.csv\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = MODEL_CONFIG\n",
    "\n",
    "TOPOLOGY_NAME = params[\"model_name\"]\n",
    "STACKED = False #Set this when loading a stacked AE\n",
    "\n",
    "FEATURE_SELECTION = False\n",
    "REPEAT_SUPERVISED = 1\n",
    "PERCENTAGE = 0\n",
    "\n",
    "model_config = readModelConfig(conf,exp_name=EXP_NAME,cv_index=CV_INDEX,model_name=TOPOLOGY_NAME,system=SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a68efb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert test data to array format\n",
    "assert list(test_label.index) == list(test_data.index.get_level_values('node_id')) \n",
    "test_true_data = test_data.values \n",
    "#test_true_data_fs = test_data[list(selected_features['0'].values)] \n",
    "test_true_label = test_label['anom'].values \n",
    "\n",
    "#Training data with feature selection\n",
    "#train_data_fs = train_data[list(selected_features['0'].values)]\n",
    "X_DIM = train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "83f411a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_labeled_pool contains one sample from each application anomaly pair\n",
    "initial_labeled_pool = train_label[(train_label['label_status'] != -1)]\n",
    "\n",
    "#Active learning or random sampling will be querying from the same pool\n",
    "initial_unlabeled_pool = train_label[(train_label['label_status'] == -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "973f0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMultipleTrainTestsProctorScikit(name,model_config,\n",
    "                                       topology,X_train,y_train,X_test,y_test,\n",
    "                                       repeat,cv_index,size,multimodal=True,stacked=False):\n",
    "      \n",
    "                                      \n",
    "        macro_fscore = 0\n",
    "        weight_fscore = 0\n",
    "        miss_rate = 0\n",
    "        alarm_rate = 0\n",
    "        score_dict = {}\n",
    "        \n",
    "        #It means loading a full autoencoder\n",
    "        if not stacked:\n",
    "            reconstructedModel = LoadModel(model_config,topology)\n",
    "            reconstructedModel = reconstructedModel.encoder\n",
    "        else:\n",
    "            reconstructedModel = LoadModel(model_config,topology)\n",
    "                \n",
    "        #Test data is constant \n",
    "        hidden_test = reconstructedModel.predict(X_test)             \n",
    "                \n",
    "        param_grid = {'C': [0.1,0.5, 1, 5, 10, 100, 1000],  \n",
    "                      'tol': [1e-4, 1e-5, 1e-6,1e-7], \n",
    "                      'class_weight': [None, 'balanced'],\n",
    "                      'max_iter': [1000,3000,5000,10000],\n",
    "                      'penalty': ['l1','l2']\n",
    "                      \n",
    "                      }  \n",
    "                    \n",
    "        for i in range(repeat):                                    \n",
    "            \n",
    "            if multimodal:\n",
    "                \n",
    "                #Only anom vs normal            \n",
    "                train_binary_label = y_train.copy()\n",
    "                train_binary_label[train_binary_label != 0] = 1  \n",
    "                                \n",
    "                hidden_train_binary = reconstructedModel.predict(X_train)\n",
    "                            \n",
    "                \n",
    "                ####Previous\n",
    "                clf_binary = svm.LinearSVC(max_iter=10000,C=100)                 \n",
    "                clf_binary.fit(hidden_train_binary, train_binary_label)                \n",
    "                \n",
    "                #Only anomalies\n",
    "                anom_indices = np.where(y_train != 0)\n",
    "                train_anom_label = y_train[anom_indices]\n",
    "                train_anom_data = X_train[anom_indices]  \n",
    "                \n",
    "                hidden_train_anom = reconstructedModel.predict(train_anom_data)\n",
    "                               \n",
    "                clf_anom = svm.LinearSVC(max_iter=10000,C=100)    \n",
    "                clf_anom.fit(hidden_train_anom, train_anom_label)                \n",
    "                \n",
    "                test_pred_label = filteredTestingProctorScikit(clf_binary,clf_anom,hidden_test)\n",
    "            \n",
    "            else:\n",
    "                if name == 'aksar_svm':\n",
    "                    hidden_train = reconstructedModel.predict(X_train)\n",
    "                    \n",
    "                    #clf = svm.LinearSVC(max_iter=10000,C=100)  \n",
    "                    #clf = svm.LinearSVC()    \n",
    "                    clf = svm.SVC()\n",
    "                    clf.fit(hidden_train, y_train)                            \n",
    "                    test_pred_label = clf.predict(hidden_test)\n",
    "                    \n",
    "                elif name == 'aksar_lr':      \n",
    "                    \n",
    "                    hidden_train = reconstructedModel.predict(X_train)\n",
    "                    clf = LogisticRegression(random_state=1234,\n",
    "                                                    dual=True,\n",
    "                                                    penalty='l2',\n",
    "                                                    solver='liblinear')                      \n",
    "                    clf.fit(hidden_train, y_train)                            \n",
    "                    test_pred_label = clf.predict(hidden_test)\n",
    "                    \n",
    "                elif name == 'aksar_svm-scale':      \n",
    "                    hidden_train = reconstructedModel.predict(X_train)\n",
    "                    clf = svm.SVC(C=1,kernel='rbf',probability=True)\n",
    "                    clf.fit(hidden_train, y_train)                            \n",
    "                    test_pred_label = clf.predict(hidden_test)                                                                                             \n",
    "            \n",
    "            score_report = classification_report(y_true=y_test, y_pred =test_pred_label,output_dict=True)\n",
    "            logging.info(\"#############\")\n",
    "            logging.info(classification_report(y_true=y_test, y_pred =test_pred_label))\n",
    "            logging.info(\"#############\")            \n",
    "            alarm_report = falseAnomRateCalc(y_test,test_pred_label)\n",
    "            \n",
    "            macro_fscore += score_report['macro avg']['f1-score']            \n",
    "            weight_fscore += score_report['weighted avg']['f1-score']\n",
    "            miss_rate += alarm_report['anom_miss_rate'] \n",
    "            alarm_rate += alarm_report['false_alarm_rate']\n",
    "        \n",
    "        \n",
    "        score_dict['false_alarm_rate'] = alarm_rate / repeat\n",
    "        score_dict['anom_miss_rate'] = miss_rate / repeat        \n",
    "        score_dict['macro_fscore'] = macro_fscore / repeat\n",
    "        score_dict['weight_fscore'] = weight_fscore / repeat\n",
    "        \n",
    "        \n",
    "#         filename = f\"test_Proctor-Kmeans-LogisticRegression-NoLP-NoTH_5_{size}_{cv_index}.json\"\n",
    "#         json_dump = json.dumps(score_dict)\n",
    "#         f_json = open(model_config['results_dir'] / (filename),\"w\")\n",
    "#         f_json.write(json_dump)\n",
    "#         f_json.close() \n",
    "        \n",
    "        return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeddafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 10 #Select 50 samples at a time\n",
    "\n",
    "for repeat_num in range(0,10):\n",
    "\n",
    "    macro_f1_scores = []\n",
    "    anomaly_miss_rates = []\n",
    "    false_alarm_rates = []\n",
    "    \n",
    "    \n",
    "    temp_labeled_pool_labels = initial_labeled_pool.copy()\n",
    "    temp_unlabeled_pool_labels = initial_unlabeled_pool.copy()\n",
    "\n",
    "    logging.info(\"Temp Labeled data label dist: \\n%s\",temp_labeled_pool_labels['anom'].value_counts())\n",
    "    logging.info(\"Temp Unlabeled data label dist: \\n%s\",temp_unlabeled_pool_labels['anom'].value_counts())    \n",
    "    \n",
    "    train_semisup_data = train_data.loc[temp_labeled_pool_labels.index]\n",
    "    train_semisup_label = temp_labeled_pool_labels['anom'].values    \n",
    "        \n",
    "    temp_score_dict = runMultipleTrainTestsProctorScikit(      \n",
    "                        'aksar_svm',\n",
    "                        model_config,\n",
    "                        TOPOLOGY_NAME,\n",
    "                        train_semisup_data,\n",
    "                        train_semisup_label,\n",
    "                        test_true_data,\n",
    "                        test_true_label,\n",
    "                        REPEAT_SUPERVISED,\n",
    "                        CV_INDEX,\n",
    "                        num_samples_per_pair,                    \n",
    "                        multimodal=False, \n",
    "                        stacked=STACKED)        \n",
    "\n",
    "\n",
    "    macro_f1_scores.append(temp_score_dict['macro_fscore'])                                        \n",
    "    false_alarm_rates.append(temp_score_dict['false_alarm_rate'])                                        \n",
    "    anomaly_miss_rates.append(temp_score_dict['anom_miss_rate'])      \n",
    "\n",
    "\n",
    "    for i in range(0,query_size,step):\n",
    "\n",
    "        logging.info(f\"Labeled Pool Size: {len(temp_labeled_pool_labels)};  Unlabeled Pool Size: {len(temp_unlabeled_pool_labels)} \\n\", )\n",
    "\n",
    "        temp_selected_labels = temp_unlabeled_pool_labels.sample(step)\n",
    "        temp_labeled_pool_labels = pd.concat([temp_labeled_pool_labels, temp_selected_labels])\n",
    "        temp_unlabeled_pool_labels.drop(index=temp_selected_labels.index,inplace=True)\n",
    "\n",
    "        logging.info(\"Temp Labeled data label dist: \\n%s\",temp_labeled_pool_labels['anom'].value_counts())\n",
    "        logging.info(\"Temp Unlabeled data label dist: \\n%s\",temp_unlabeled_pool_labels['anom'].value_counts())\n",
    "\n",
    "        train_semisup_data = train_data.loc[temp_labeled_pool_labels.index]\n",
    "        train_semisup_label = temp_labeled_pool_labels['anom'].values\n",
    "\n",
    "\n",
    "        temp_score_dict = runMultipleTrainTestsProctorScikit(      \n",
    "                            'aksar_svm',\n",
    "                            model_config,\n",
    "                            TOPOLOGY_NAME,\n",
    "                            train_semisup_data,\n",
    "                            train_semisup_label,\n",
    "                            test_true_data,\n",
    "                            test_true_label,\n",
    "                            REPEAT_SUPERVISED,\n",
    "                            CV_INDEX,\n",
    "                            num_samples_per_pair,                    \n",
    "                            multimodal=False, \n",
    "                            stacked=STACKED)        \n",
    "\n",
    "\n",
    "        macro_f1_scores.append(temp_score_dict['macro_fscore'])                                        \n",
    "        false_alarm_rates.append(temp_score_dict['false_alarm_rate'])                                        \n",
    "        anomaly_miss_rates.append(temp_score_dict['anom_miss_rate'])      \n",
    "        \n",
    "        scores = pd.DataFrame()\n",
    "\n",
    "        for j in range(0,len(macro_f1_scores)):\n",
    "            scores = scores.append({'query_iter':j*step,\n",
    "                                    'macro_avg_f1_score':macro_f1_scores[j],\n",
    "                                    'false_alarm_rate':false_alarm_rates[j],\n",
    "                                    'anomaly_miss_rate':anomaly_miss_rates[j], \n",
    "                                    'repeat_num':repeat_num},\n",
    "                                   ignore_index = True)\n",
    "\n",
    "\n",
    "        scores['fold'] = CV_INDEX\n",
    "        scores['method'] = method\n",
    "        scores['query_strategy'] = query_strategy\n",
    "        scores['model'] = \"Proctor\"\n",
    "        scores['dataset'] = SYSTEM\n",
    "        scores['fe'] = FE_NAME\n",
    "        scores['feature_count'] = NUM_FEATURE   \n",
    "        scores['query_size'] = query_size\n",
    "\n",
    "\n",
    "        scores = scores.sort_values(by = ['query_iter']).reset_index(drop = True)\n",
    "\n",
    "        train_app_names = \"all\"\n",
    "        test_app_names = \"all\"\n",
    "\n",
    "        filename = f\"train:{train_app_names}#test:{test_app_names}#{FE_NAME}#{NUM_FEATURE}#{method}#{query_strategy}#{query_size}#Proctor#{repeat_num}.csv\"\n",
    "        scores.to_csv(Path(conf[\"results_dir\"]) / filename)\n",
    "\n",
    "        logging.info(\"Saving: %s\", filename)                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900888e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d9d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2ad53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
