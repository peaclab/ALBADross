{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebfeda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd7f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b12076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sns.set_context('paper')\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-7s %(message)s',\n",
    "                    stream=sys.stderr, level=logging.INFO)\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.INFO)\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "#In-house Module Imports\n",
    "from config import Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daccf707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_analysis_line_plots(stats_df,param_dict,naming_dict, color_dict, use_std=True):\n",
    "    \n",
    "    strategies = stats_df['query_strategy'].unique()\n",
    "    \n",
    "    #fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "    fig, ax = plt.subplots(1,3,figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "\n",
    "    for strategy in strategies:\n",
    "        selected_score_data = stats_df.loc[stats_df['query_strategy'] == strategy]\n",
    "        x = selected_score_data['query_iter']\n",
    "        y = selected_score_data['mean']\n",
    "        ax[0].plot(x, \n",
    "                 y,\n",
    "                 linewidth=3,\n",
    "                 color=color_dict[strategy],\n",
    "                 label=strategy)\n",
    "        if use_std:\n",
    "            ax[0].fill_between(x, \n",
    "                             y - (0.5*selected_score_data['std']), \n",
    "                             y + (0.5*selected_score_data['std']),\n",
    "                             alpha=0.2,  \n",
    "                             color=color_dict[strategy],\n",
    "                            )\n",
    "        else:\n",
    "            \n",
    "            ax[0].fill_between(x, \n",
    "                 selected_score_data['ci95_lo'], \n",
    "                 selected_score_data['ci95_hi'],\n",
    "                 alpha=1, \n",
    "                 color=color_dict[strategy],\n",
    "                )\n",
    "\n",
    "    legend = ax[0].legend(title=\"\",prop={'size': param_dict['legend_size']})\n",
    "    \n",
    "    ax[0].title(f\"{naming_dict['title_prefix']} w.r.t Number of Queries \\n {naming_dict['fe_name']} - {naming_dict['system']} - {naming_dict['num_features']} \\n #Known Apps in Training: {naming_dict['num_train_known_apps']}\",fontsize=param_dict['title_size'])\n",
    "    #plt.ylim([0.3, 1.01])\n",
    "    ax[0].xlim([0, 250])\n",
    "\n",
    "    ax[0].ylabel(f\"{naming_dict['title_prefix']}\",size=param_dict['y_label_font'])\n",
    "    ax[0].xlabel(\"Number of Iterations\",size=param_dict['x_label_font'])\n",
    "    ax[0].xticks(fontsize=param_dict['x_ticks_font'])\n",
    "    ax[0].yticks(fontsize=param_dict['y_ticks_font'])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56713cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_query_line_plot(stats_df,param_dict,naming_dict, color_dict, use_std=True):\n",
    "    \n",
    "    strategies = stats_df['query_strategy'].unique()\n",
    "\n",
    "    fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "\n",
    "    for strategy in strategies:\n",
    "        selected_score_data = stats_df.loc[stats_df['query_strategy'] == strategy]\n",
    "        x = selected_score_data['query_iter']\n",
    "        y = selected_score_data['mean']\n",
    "        plt.plot(x, \n",
    "                 y,\n",
    "                 linewidth=3,\n",
    "                 color=color_dict[strategy],\n",
    "                 label=strategy)\n",
    "        if use_std:\n",
    "            plt.fill_between(x, \n",
    "                             y - (0.5*selected_score_data['std']), \n",
    "                             y + (0.5*selected_score_data['std']),\n",
    "                             alpha=0.2,  \n",
    "                             color=color_dict[strategy],\n",
    "                            )\n",
    "        else:\n",
    "            \n",
    "            plt.fill_between(x, \n",
    "                 selected_score_data['ci95_lo'], \n",
    "                 selected_score_data['ci95_hi'],\n",
    "                 alpha=1, \n",
    "                 color=color_dict[strategy],\n",
    "                )\n",
    "\n",
    "    legend = plt.legend(title=\"\",prop={'size': param_dict['legend_size']})\n",
    "    \n",
    "    plt.title(f\"{naming_dict['title_prefix']} w.r.t Number of Queries \\n {naming_dict['fe_name']} - {naming_dict['system']} - {naming_dict['num_features']} \\n #Known Apps in Training: {naming_dict['num_train_known_apps']}\",fontsize=param_dict['title_size'])\n",
    "    #plt.ylim([0.3, 1.01])\n",
    "    plt.xlim([0, 250])\n",
    "\n",
    "    plt.ylabel(f\"{naming_dict['title_prefix']}\",size=param_dict['y_label_font'])\n",
    "    plt.xlabel(\"Number of Iterations\",size=param_dict['x_label_font'])\n",
    "    plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "    plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f394d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(df, score_type):\n",
    "    \n",
    "    stats = df.groupby(['query_strategy','query_iter'])[score_type].agg(['mean', 'count', 'std'])\n",
    "\n",
    "    ci95_hi = []\n",
    "    ci95_lo = []\n",
    "    z_star = 1.96 # 1.65: ci90\n",
    "\n",
    "    for i in stats.index:\n",
    "        m, c, s = stats.loc[i]#[2:5]\n",
    "        ci95_hi.append(m + z_star*s/math.sqrt(c))\n",
    "        ci95_lo.append(m - z_star*s/math.sqrt(c))\n",
    "\n",
    "    stats['ci95_hi'] = ci95_hi\n",
    "    stats['ci95_lo'] = ci95_lo\n",
    "\n",
    "    stats.reset_index(['query_strategy','query_iter'], inplace=True)    \n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68b3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update these\n",
    "PARENT_DIR = 'active_learning_experiments'\n",
    "SYSTEM = 'volta'\n",
    "FE_NAME = 'tsfresh'\n",
    "EXP_NAME = f'{FE_NAME}_experiments'\n",
    "DIR_NAME_TO_GENERATE_RESULTS = 'exp_2_active_learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c01b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f'/projectnb/peaclab-mon/{user}/{PARENT_DIR}' # or feature_extraction_experiments\n",
    "CV_INDEX = 0\n",
    "SCALER = 'None' #For now, do the scaling inside the notebook, then you can move that to the class function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Configuration(ipython=True,\n",
    "                     overrides={\n",
    "                         'output_dir': Path(OUTPUT_DIR), #change\n",
    "                         'system' : SYSTEM,\n",
    "                         'exp_name':EXP_NAME,                                                  \n",
    "                         'cv_fold':CV_INDEX, \n",
    "                         'model_config': DIR_NAME_TO_GENERATE_RESULTS\n",
    "                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "counter = 0\n",
    "result_list = []\n",
    "\n",
    "for cv_index in [0,1,2,3,4]:\n",
    "\n",
    "    conf = Configuration(ipython=True,\n",
    "                         overrides={\n",
    "                             'output_dir': Path(OUTPUT_DIR), #change\n",
    "                             'system' : SYSTEM,\n",
    "                             'exp_name':EXP_NAME,                                                  \n",
    "                             'cv_fold':cv_index, \n",
    "                             'model_config': DIR_NAME_TO_GENERATE_RESULTS\n",
    "                         }\n",
    "                        )\n",
    "\n",
    "    for filename in os.listdir(conf['results_dir']):\n",
    "        \n",
    "        if  not \"app-anom-selection\" in filename:\n",
    "            \n",
    "            only_filename = filename.split('.')\n",
    "            splitted_filename = only_filename[0].split('#')\n",
    "            train_apps = splitted_filename[0].split(':')[1]\n",
    "            test_apps = splitted_filename[1].split(':')[1]\n",
    "            num_unknown_test_apps = len(test_apps.split(\"-\"))\n",
    "            num_known_train_apps = len(train_apps.split(\"-\"))        \n",
    "\n",
    "            temp_csv = pd.read_csv(Path(conf['results_dir']) / filename,index_col = [0])\n",
    "            temp_csv['train_apps'] = train_apps\n",
    "            temp_csv['test_apps'] = test_apps        \n",
    "            temp_csv['num_unknown_test_apps'] = num_unknown_test_apps\n",
    "            temp_csv['num_known_train_apps'] = num_known_train_apps\n",
    "            temp_csv['repeat_num'] = int(splitted_filename[8])\n",
    "\n",
    "            result_list.append(temp_csv)\n",
    "\n",
    "            counter += 1\n",
    "               \n",
    "    result_df = pd.concat(result_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_query_strategies = result_df['query_strategy'].unique()\n",
    "logging.info(\"Unique query strategies: %s\",unique_query_strategies)\n",
    "unique_methods = result_df['method'].unique()\n",
    "logging.info(\"Unique Methods: %s\", unique_methods)\n",
    "unique_fe_methods = result_df['fe'].unique()\n",
    "logging.info(\"Feature Extraction Methods: %s\",unique_fe_methods)\n",
    "unique_feature_counts = sorted(result_df['feature_count'].unique())\n",
    "logging.info(\"Num Features: %s\", unique_feature_counts)\n",
    "unique_query_sizes = sorted(result_df['query_size'].unique())\n",
    "logging.info(\"Unique query sizes: %s\",unique_query_sizes)\n",
    "unique_known_train_apps = result_df['num_known_train_apps'].unique()\n",
    "logging.info(\"Number of Known Apps in the Training: %s\", unique_known_train_apps)\n",
    "\n",
    "# unique_random_selection = len(result_df[result_df['query_strategy'] == 'random']['repeat_num'].unique())\n",
    "# logging.info(\"Number of Repeats for the Random Selection: %s\", unique_random_selection)\n",
    "\n",
    "unique_folds = len(result_df[result_df['query_strategy'] != 'random']['fold'].unique())\n",
    "logging.info(\"Number of Folds for the Active Learning Methods: %s\", unique_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ae816",
   "metadata": {},
   "source": [
    "## Paper Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "405d3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "                'entropy': 'tab:purple',\n",
    "                'margin': 'tab:blue',    \n",
    "                'uncertainty': 'orange',    \n",
    "                'random': 'tab:green',\n",
    "                'equal_app' : 'tab:red',\n",
    "             }\n",
    "\n",
    "param_dict = {\n",
    "                'fig_width': 48,\n",
    "                'fig_height': 12,\n",
    "                'y_label_font': 40,\n",
    "                'x_label_font': 40,\n",
    "                'x_ticks_font': 37,\n",
    "                'y_ticks_font': 37,    \n",
    "                'legend_size': 42,\n",
    "                'legend_title_size': 40,\n",
    "                'title_size': 46,\n",
    "                'title_pad': 40, \n",
    "                'fig_title_size': 50,\n",
    "    \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08fc6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'RandomForestClassifier'\n",
    "num_query = 250\n",
    "feature_count = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "247b4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict = {}\n",
    "naming_dict['fe_name'] = FE_NAME\n",
    "naming_dict['system'] = SYSTEM\n",
    "naming_dict['num_features'] = int(feature_count)\n",
    "naming_dict['num_queries'] = num_query  \n",
    "naming_dict['model'] = selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dadab527",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_result_df = result_df[(result_df['query_size'] == num_query) & (result_df['model'] == selected_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_2_result_df = selected_result_df[selected_result_df['num_known_train_apps'] == 2]\n",
    "known_4_result_df = selected_result_df[selected_result_df['num_known_train_apps'] == 4]\n",
    "known_6_result_df = selected_result_df[selected_result_df['num_known_train_apps'] == 6]\n",
    "\n",
    "# stats_far = calculate_stats(selected_result_df, 'false_alarm_rate')\n",
    "# stats_amr = calculate_stats(selected_result_df, 'anomaly_miss_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d0907d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(data=mg_CoMD, x=\"macro_avg_f1_score\", y=\"query_iter\", hue=\"fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced478aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3bab18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_2_stats_fscore = calculate_stats(known_2_result_df, 'macro_avg_f1_score')\n",
    "known_4_stats_fscore = calculate_stats(known_4_result_df, 'macro_avg_f1_score')\n",
    "known_6_stats_fscore = calculate_stats(known_6_result_df, 'macro_avg_f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "417a0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_query_line_plot(fscore_df, far_df, amr_df, param_dict, naming_dict, color_dict, num_query=50, use_std=False):\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(1,3,figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "    \n",
    "    for ind, (stats_df,fig_title, fig_y_label) in enumerate(zip([fscore_df,far_df,amr_df],[\"Training Dataset: 2 Apps\",\"Training Dataset: 4 Apps\",\"Training Dataset: 6 Apps\"],[\"F1-score (Macro Avg)\",\"\",\"\"])):\n",
    "        \n",
    "        strategies = stats_df['query_strategy'].unique()\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            \n",
    "            selected_score_data = stats_df.loc[stats_df['query_strategy'] == strategy]\n",
    "            x = selected_score_data['query_iter']\n",
    "            y = selected_score_data['mean']\n",
    "            axs[ind].plot(x, \n",
    "                     y,\n",
    "                     linewidth=3,\n",
    "                     color=color_dict[strategy],\n",
    "                     label=strategy)\n",
    "            if use_std:\n",
    "                axs[ind].fill_between(x, \n",
    "                                 y - (1*selected_score_data['std']), \n",
    "                                 y + (1*selected_score_data['std']),\n",
    "                                 alpha=0.2,  \n",
    "                                 color=color_dict[strategy],\n",
    "                                )\n",
    "            else:\n",
    "\n",
    "                axs[ind].fill_between(x, \n",
    "                     selected_score_data['ci95_lo'], \n",
    "                     selected_score_data['ci95_hi'],\n",
    "                     alpha=0.1, \n",
    "                     color=color_dict[strategy],\n",
    "                    )\n",
    "                \n",
    "        if ind == 0:\n",
    "            axs[ind].axvline(x=50, linewidth=4, color='xkcd:black', linestyle='--', label=\"$Min Query_{F1_{95}}$\")                      \n",
    "        elif ind == 1:\n",
    "            axs[ind].axvline(x=35, linewidth=4, color='xkcd:black', linestyle='--', label=\"$Min Query_{F1_{95}}$\")                      \n",
    "        elif ind == 2:\n",
    "            axs[ind].axvline(x=30, linewidth=4, color='xkcd:black', linestyle='--', label=\"$Min Query_{F1_{95}}$\")                      \n",
    "\n",
    "            \n",
    "        if naming_dict['system'] == 'eclipse':\n",
    "            axs[ind].axhline(y=0.95, linewidth=4, color='r', linestyle='--', label='$F1_{95}$')                 \n",
    "        elif naming_dict['system'] == 'volta':\n",
    "            axs[ind].axhline(y=0.95, linewidth=4, color='r', linestyle='--', label='$F1_{95}$') \n",
    "            \n",
    "            \n",
    "        axs[ind].set_title(fig_title, fontsize=param_dict['title_size'])     \n",
    "        axs[ind].set_xlim([0, num_query])\n",
    "        \n",
    "        axs[ind].set_ylabel(f\"{fig_y_label}\",size=param_dict['y_label_font'])\n",
    "        axs[ind].set_xlabel(\"Number of Queries\",size=param_dict['x_label_font'])\n",
    "        axs[ind].tick_params(axis='x', labelsize=param_dict['x_ticks_font'])\n",
    "        axs[ind].tick_params(axis='y', labelsize=param_dict['y_ticks_font'])                \n",
    "    \n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, [\"Random\",\"Uncertainty\", \"$Min Query_{F1_{95}}$\", \"$F1_{95}$\"], loc='lower left', \n",
    "               bbox_to_anchor=(0.27, 1.0, 0.3, 0.4), ncol=5, frameon=True, mode='None',\n",
    "               prop={'size': param_dict['legend_size']}\n",
    "              )\n",
    "        \n",
    "    #fig.suptitle(f\"Active Learning with {naming_dict['model']}\",fontsize=param_dict['fig_title_size'])        \n",
    "    fig.suptitle(f\"Previously Unseen Applications - {naming_dict['system'].capitalize()}\",fontsize=param_dict['fig_title_size'], y = 1.20) \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"/usr3/graduate/baksar/projectx/AI4HPCAnalytics/src/active_learning_experiments/plots/{naming_dict['system']}_{naming_dict['fe_name']}_unseen_apps.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_query_line_plot(known_2_stats_fscore, known_4_stats_fscore, known_6_stats_fscore, \n",
    "                              param_dict, \n",
    "                              naming_dict, \n",
    "                              color_dict, \n",
    "                              num_query=num_query,\n",
    "                              use_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a5296",
   "metadata": {},
   "source": [
    "## Plot for Each Feature Extraction Method and Rates Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b706ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_known_apps = 8\n",
    "feature_count = unique_feature_counts[0]\n",
    "temp_result_df = result_df[result_df['num_known_train_apps'] == num_train_known_apps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "642aa5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_fscore = calculate_stats(temp_result_df, 'macro_avg_f1_score')\n",
    "stats_far = calculate_stats(temp_result_df, 'false_alarm_rate')\n",
    "stats_amr = calculate_stats(temp_result_df, 'anomaly_miss_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57f6fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict = {}\n",
    "naming_dict['fe_name'] = FE_NAME\n",
    "naming_dict['system'] = SYSTEM\n",
    "naming_dict['num_features'] = int(feature_count)\n",
    "naming_dict['num_queries'] = unique_query_sizes[0]\n",
    "naming_dict['num_train_known_apps'] = int(num_train_known_apps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc5cc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "                'entropy': 'tab:purple',\n",
    "                'margin': 'tab:blue',    \n",
    "                'uncertainty': 'orange',    \n",
    "                'random': 'tab:green',\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a8a8118",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "                'fig_width': 18,\n",
    "                'fig_height': 12,\n",
    "                'y_label_font': 45,\n",
    "                'x_label_font': 45,\n",
    "                 'x_ticks_font': 42,\n",
    "                 'y_ticks_font': 50,    \n",
    "                'legend_size': 30,\n",
    "                'legend_title_size': 40,\n",
    "                'title_size': 36,\n",
    "                'title_pad': 40,             \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01485765",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict['title_prefix'] = \"F1-Score (Macro Avg)\"\n",
    "plot_query_line_plot(stats_fscore, param_dict, naming_dict, color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8241d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict['title_prefix'] = \"False Alarm Rate\"\n",
    "plot_query_line_plot(stats_far, param_dict, naming_dict, color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec676f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "naming_dict['title_prefix'] = \"Anomaly Miss Rate\"\n",
    "plot_query_line_plot(stats_amr, param_dict, naming_dict, color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e648f",
   "metadata": {},
   "source": [
    "## Old Slow Plot Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_count in unique_feature_counts:\n",
    "    \n",
    "    for num_train_known_apps in unique_known_train_apps:\n",
    "        \n",
    "        sub_df = result_df[(result_df['feature_count'] == feature_count) & (result_df['num_known_train_apps'] == num_train_known_apps)]\n",
    "        sub_df = sub_df.sort_values(['method','query_strategy'])\n",
    "        \n",
    "        fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "        ax = sns.lineplot(x=\"query_iter\", y=\"macro_avg_f1_score\",\n",
    "                          sort= False,\n",
    "                          hue = 'query_strategy',\n",
    "                          markers=True,\n",
    "                          data=sub_df)\n",
    "\n",
    "        legend = plt.legend(title=\"\",prop={'size': param_dict['legend_size']})\n",
    "\n",
    "        ax.set_title(f'Macro Avg F1-score w.r.t Number of Queries \\n {FE_NAME} - {SYSTEM} - {int(feature_count)} features \\n #Known Apps in Training: {num_train_known_apps}',fontsize=param_dict['title_size'])\n",
    "        ax.set(ylim=(0.3, 1.01))\n",
    "\n",
    "        ax.set_ylabel('F1-score',size=param_dict['y_label_font'])\n",
    "        ax.set_xlabel(\"Number of Iterations\",size=param_dict['x_label_font'])\n",
    "\n",
    "        plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "        plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "\n",
    "\n",
    "        plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "274696b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_count in unique_feature_counts:\n",
    "    \n",
    "    for num_train_known_apps in unique_known_train_apps:\n",
    "        \n",
    "        sub_df = result_df[(result_df['feature_count'] == feature_count) & (result_df['num_known_train_apps'] == num_train_known_apps)]\n",
    "        sub_df = sub_df.sort_values(['method','query_strategy'])\n",
    "        \n",
    "        fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "        ax = sns.lineplot(x=\"query_iter\", y=\"false_alarm_rate\",\n",
    "                          sort= False,\n",
    "                          hue = 'query_strategy',\n",
    "                          markers=True,\n",
    "                          data=sub_df)\n",
    "\n",
    "        legend = plt.legend(title=\"\",prop={'size': param_dict['legend_size']})\n",
    "\n",
    "        ax.set_title(f'False Alarm Rate w.r.t Number of Queries \\n {FE_NAME} - {SYSTEM} - {int(feature_count)} features \\n #Known Apps in Training: {num_train_known_apps}',fontsize=param_dict['title_size'])\n",
    "\n",
    "        ax.set_ylabel('False Alarm Rate',size=param_dict['y_label_font'])\n",
    "        ax.set_xlabel(\"Number of Iterations\",size=param_dict['x_label_font'])\n",
    "\n",
    "        plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "        plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "\n",
    "\n",
    "        plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22235737",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_count in unique_feature_counts:\n",
    "    \n",
    "    for num_train_known_apps in unique_known_train_apps:\n",
    "        \n",
    "        sub_df = result_df[(result_df['feature_count'] == feature_count) & (result_df['num_known_train_apps'] == num_train_known_apps)]\n",
    "        sub_df = sub_df.sort_values(['method','query_strategy'])\n",
    "        \n",
    "        fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "        ax = sns.lineplot(x=\"query_iter\", y=\"anomaly_miss_rate\",\n",
    "                          sort= False,\n",
    "                          hue = 'query_strategy',\n",
    "                          markers=True,\n",
    "                          data=sub_df)\n",
    "\n",
    "        legend = plt.legend(title=\"\",prop={'size': param_dict['legend_size']})\n",
    "\n",
    "        ax.set_title(f'Anomaly Miss Rate w.r.t Number of Queries \\n {FE_NAME} - {SYSTEM} - {int(feature_count)} features \\n #Known Apps in Training: {num_train_known_apps}',fontsize=param_dict['title_size'])\n",
    "\n",
    "        ax.set_ylabel('Anomaly Miss Rate',size=param_dict['y_label_font'])\n",
    "        ax.set_xlabel(\"Number of Iterations\",size=param_dict['x_label_font'])\n",
    "\n",
    "        plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "        plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "\n",
    "        plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c321b75",
   "metadata": {},
   "source": [
    "### Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "read_dir =  conf['experiment_dir'] / 'CV_0' / 'unseen_app_results_one_repeat' / 'results'\n",
    "\n",
    "for filename in os.listdir(read_dir):\n",
    "    splitted_filename = filename.split('#')\n",
    "        \n",
    "    if splitted_filename[4] == 'random': \n",
    "        print(filename)    \n",
    "        one_repeat_csv = pd.read_csv(read_dir / filename,index_col = [0])\n",
    "        break\n",
    "one_repeat_csv.head()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_cols = one_repeat_csv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd58643",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir =  conf['experiment_dir'] / 'CV_0' / 'unseen_app_results_one_repeat' / 'results'\n",
    "\n",
    "for filename in os.listdir(read_dir):\n",
    "    one_repeat_csv = pd.read_csv(read_dir / filename,index_col = [0])\n",
    "    if not (one_repeat_csv.columns.values == correct_cols).all():\n",
    "            print(filename)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc82081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Delete the wrong stuff\n",
    "# for filename in os.listdir(conf['results_dir']):\n",
    "#     splitted_filename = filename.split('#')\n",
    "        \n",
    "#     if splitted_filename[4] != 'random': \n",
    "#         print(filename)\n",
    "#         #os.remove(conf['results_dir'] / filename)\n",
    "#         #new_file = pd.read_csv(conf['results_dir'] / filename,index_col = [0])\n",
    "#         #break\n",
    "#new_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b78353",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir =  conf['experiment_dir'] / 'CV_0' / 'unseen_app_results_worstbest' / 'results'\n",
    "\n",
    "for filename in os.listdir(read_dir):\n",
    "    prev_orig_csv = pd.read_csv(read_dir / filename,index_col = [0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir = conf['experiment_dir'] / 'CV_0' / 'unseen_app_results_worstbest' / 'results'\n",
    "\n",
    "for filename in os.listdir(read_dir):\n",
    "    \n",
    "    splitted_filename = filename.split('#')\n",
    "    \n",
    "    if splitted_filename[4] != 'random':    \n",
    "        lolo = pd.read_csv(read_dir / filename,index_col = [0])\n",
    "        filename_splitted = filename.split('.')\n",
    "        new_filename = filename_splitted[0] + '#0.' + filename_splitted[1]    \n",
    "        num_repeat_col = lolo['num_repeat'].values\n",
    "        query_iter_col = lolo['query_iter'].values\n",
    "        lolo.drop(columns = ['num_repeat','query_iter'],inplace=True)\n",
    "        lolo.insert(3, 'query_iter', query_iter_col)\n",
    "        lolo.insert(4, 'repeat_num', num_repeat_col)        \n",
    "        #print(new_filename)\n",
    "        lolo.to_csv(conf['experiment_dir'] / 'CV_0' / 'unseen_app_results_worstbest' / 'modified_al_results' / new_filename)        \n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72291fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir =  conf['experiment_dir'] / 'CV_0' / 'unseen_app_results_worstbest' / 'modified_al_results'\n",
    "\n",
    "for filename in os.listdir(read_dir):\n",
    "    modified_csv = pd.read_csv(read_dir / filename,index_col = [0])\n",
    "    \n",
    "    if not (modified_csv.columns.values == correct_cols).all():\n",
    "        print(filename)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b931d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66181a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
