{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a80669",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee388a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "sns.set_context('paper')\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-7s %(message)s',\n",
    "                    stream=sys.stderr, level=logging.INFO)\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.INFO)\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#In-house Module Imports\n",
    "from config import Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a5f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"aksar\"\n",
    "logging.warning(f'Are you sure that you are: {user}?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3505d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update these\n",
    "\n",
    "PARENT_DIR = 'unseen_experiments'\n",
    "SYSTEM = 'volta'\n",
    "EXP_NAME = 'tpds_experiments'\n",
    "DIR_NAME_TO_GENERATE_RESULTS = 'experiment_1' #rf_tuncer_worst_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f'/projectnb/peaclab-mon/{user}/{PARENT_DIR}' # or feature_extraction_experiments\n",
    "CV_INDEX = 0\n",
    "SCALER = 'None' #For now, do the scaling inside the notebook, then you can move that to the class function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Configuration(ipython=True,\n",
    "                     overrides={\n",
    "                         'output_dir': Path(OUTPUT_DIR), #change\n",
    "                         'system' : SYSTEM,\n",
    "                         'exp_name':EXP_NAME,                                                  \n",
    "                         'cv_fold':CV_INDEX, \n",
    "                         'model_config': DIR_NAME_TO_GENERATE_RESULTS\n",
    "                     })\n",
    "\n",
    "with open(str(conf['experiment_dir']) + '/anom_dict.json') as f:\n",
    "    ANOM_DICT = json.load(f)\n",
    "with open(str(conf['experiment_dir']) + '/app_dict.json') as f:\n",
    "    APP_DICT = json.load(f)    \n",
    "    \n",
    "APP_REVERSE_DICT = {}\n",
    "for app_name, app_encoding in APP_DICT.items():\n",
    "    APP_REVERSE_DICT[app_encoding] = app_name    \n",
    "\n",
    "ANOM_REVERSE_DICT = {}\n",
    "for anom_name, anom_encoding in ANOM_DICT.items():\n",
    "    ANOM_REVERSE_DICT[anom_encoding] = anom_name  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "result_anom_list = []\n",
    "result_anom_df = pd.DataFrame()\n",
    "\n",
    "result_rate_list = []\n",
    "result_rate_df = pd.DataFrame()\n",
    "\n",
    "read_prefix_names = set()\n",
    "\n",
    "for cv_index in [0]:\n",
    "\n",
    "    conf = Configuration(ipython=True,\n",
    "                         overrides={\n",
    "                             'output_dir': Path(OUTPUT_DIR), #change\n",
    "                             'system' : SYSTEM,\n",
    "                             'exp_name':EXP_NAME,                                                  \n",
    "                             'cv_fold':cv_index, \n",
    "                             'model_config': DIR_NAME_TO_GENERATE_RESULTS\n",
    "                         })\n",
    "\n",
    "    for filename in os.listdir(conf['results_dir']):\n",
    "        \n",
    "        prefix_filename = \"#\".join(filename.split(\"#\")[:-1])\n",
    "        \n",
    "        if not prefix_filename in read_prefix_names:\n",
    "            \n",
    "            read_prefix_names.add(prefix_filename)\n",
    "\n",
    "            filename = filename.split('.')[0]\n",
    "            splitted_filename = filename.split('#')\n",
    "            train_apps = splitted_filename[0].split(':')[1]\n",
    "            test_apps = splitted_filename[1].split(':')[1]\n",
    "            num_unknown_test_apps = len(test_apps.split(\"-\"))\n",
    "            num_known_train_apps = len(train_apps.split(\"-\"))\n",
    "            model_name = splitted_filename[2].split(':')[1]\n",
    "            cv_fold = splitted_filename[3].split(':')[1]     \n",
    "            \n",
    "            result_row = {\n",
    "                \"train_apps\": train_apps,\n",
    "                \"test_apps\": test_apps,\n",
    "                \"num_unknown_test_apps\": int(num_unknown_test_apps),\n",
    "                \"num_known_train_apps\": int(num_known_train_apps),\n",
    "                \"model_name\": model_name,\n",
    "                \"cv_fold\": cv_fold\n",
    "            }\n",
    "\n",
    "            with open(Path(conf['results_dir']) / (prefix_filename + \"#result_dict.json\"), 'r') as f:\n",
    "\n",
    "                result_json = json.load(f)  \n",
    "\n",
    "                result_row['macro_f1_score'] = result_json['macro avg']['f1-score']\n",
    "                result_row['weighted_f1_score'] = result_json['weighted avg']['f1-score']\n",
    "                \n",
    "                result_list.append(result_row)\n",
    "                #result_df = result_df.append(result_row, ignore_index=True)\n",
    "                \n",
    "                for anom_name in list(ANOM_DICT.keys()):\n",
    "                    for score_name in ['f1-score','precision','recall']:\n",
    "                        result_anom_row = {\n",
    "                                \"train_apps\": train_apps,\n",
    "                                \"test_apps\": test_apps,\n",
    "                                \"num_unknown_test_apps\": int(num_unknown_test_apps),\n",
    "                                \"num_known_train_apps\": int(num_known_train_apps),\n",
    "                                \"anom_name\" : anom_name,\n",
    "                                \"score_name\" : score_name,\n",
    "                                \"score\" :result_json[anom_name][score_name],\n",
    "                                \"cv_fold\": cv_fold\n",
    "                        }     \n",
    "                        result_anom_list.append(result_anom_row)                       \n",
    "                        #result_anom_df = result_anom_df.append(result_anom_row, ignore_index=True)                       \n",
    "\n",
    "            with open(Path(conf['results_dir']) / (prefix_filename + \"#alert_dict.json\"), 'r') as f:\n",
    "\n",
    "                alert_json = json.load(f)  \n",
    "                                \n",
    "                for rate_name in [\"false_alarm_rate\",\"anom_miss_rate\"]:\n",
    "                    \n",
    "                    result_rate_row = {\n",
    "                            \"train_apps\": train_apps,\n",
    "                            \"test_apps\": test_apps,\n",
    "                            \"num_unknown_test_apps\": int(num_unknown_test_apps),\n",
    "                            \"num_known_train_apps\": int(num_known_train_apps),\n",
    "                            \"score_name\" : rate_name,\n",
    "                            \"score\" :alert_json[rate_name],\n",
    "                            \"cv_fold\": cv_fold\n",
    "                    }\n",
    "                    result_rate_list.append(result_rate_row)\n",
    "                    #result_rate_df = result_rate_df.append(result_rate_row, ignore_index=True)\n",
    "\n",
    "                    \n",
    "result_df = pd.DataFrame.from_dict(result_list)\n",
    "result_rate_df = pd.DataFrame.from_dict(result_rate_list)\n",
    "result_anom_df = pd.DataFrame.from_dict(result_anom_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef77420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_test_app_combos = result_df['test_apps'].unique()\n",
    "\n",
    "if SYSTEM == 'volta':\n",
    "    all_apps = ['kripke','miniMD','lu','miniAMR','sp','ft','bt','cg','miniGhost','mg','CoMD']\n",
    "    \n",
    "elif SYSTEM == 'eclipse':\n",
    "    all_apps = ['LAMMPS','sw4lite', 'examiniMD', 'swfft', 'HACC', 'sw4']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2d3bd",
   "metadata": {},
   "source": [
    "## Sort the Best and Worst 2 Train App Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f04d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_top_results = result_df[result_df['num_known_train_apps'] == 2].sort_values(by=['macro_f1_score'])\n",
    "analyze_top_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_and_best_combos = pd.concat([analyze_top_results.iloc[0:100],analyze_top_results.iloc[-100:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b472bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_and_best_combos.to_csv(\"/projectnb/peaclab-mon/aksar/active_learning_experiments/volta/mvts_experiments/worst_best_app_combos.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4556ac3",
   "metadata": {},
   "source": [
    "## Analyze the outlier cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e178190",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "                'fig_width': 18,\n",
    "                'fig_height': 12,\n",
    "                'y_label_font': 45,\n",
    "                'x_label_font': 45,\n",
    "                 'x_ticks_font': 42,\n",
    "                 'y_ticks_font': 50,    \n",
    "                'legend_size': 26,\n",
    "                'legend_title_size': 55,\n",
    "                'title_size': 36,\n",
    "                'title_pad': 40,             \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ab918",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['num_known_train_apps'] = result_df['num_known_train_apps'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "ax = sns.scatterplot(data=result_df, x=\"num_known_train_apps\", y=\"macro_f1_score\", \n",
    "                     hue=\"num_known_train_apps\",\n",
    "                     style=\"num_known_train_apps\",\n",
    "                     s=250\n",
    "                    )\n",
    "\n",
    "ax.set_ylabel('F1-score',size=param_dict['y_label_font'])\n",
    "ax.set_xlabel(\"Number of Known Train Apps\",size=param_dict['x_label_font'])\n",
    "\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "\n",
    "plt.tight_layout()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_known_train_apps in sorted(result_df['num_known_train_apps'].unique()):\n",
    "    \n",
    "    temp_score_df = result_df[result_df['num_known_train_apps'] == num_known_train_apps].sort_values(by=['macro_f1_score'])\n",
    "    fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "    temp_total = len(temp_score_df)\n",
    "    \n",
    "    temp_mean = temp_score_df['macro_f1_score'].mean()\n",
    "    temp_std = temp_score_df['macro_f1_score'].std()\n",
    "    two_std_below = temp_mean - 2*temp_std\n",
    "    one_std_below = temp_mean - temp_std    \n",
    "    one_std_below_vals = len(temp_score_df['macro_f1_score'].values[temp_score_df['macro_f1_score'].values < one_std_below])\n",
    "    two_std_below_vals = len(temp_score_df['macro_f1_score'].values[temp_score_df['macro_f1_score'].values < two_std_below])    \n",
    "\n",
    "    ax = sns.histplot(data=temp_score_df, x=\"macro_f1_score\")    \n",
    "    \n",
    "    ax.text(0.15, 0.9,f'$\\mu$:{np.round(temp_mean,2)} $\\sigma$:{np.round(temp_std,2)} \\n One $\\sigma$ below: %{np.round(100*(one_std_below_vals/temp_total),2)} \\n Two $\\sigma$ below: %{np.round(100*(two_std_below_vals/temp_total),2)}', \n",
    "            bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10},\n",
    "            fontsize=25,\n",
    "            ha='center', va='center', transform=ax.transAxes)    \n",
    "\n",
    "    ax.set_ylabel('Count',size=param_dict['y_label_font'])\n",
    "    ax.set_xlabel(\"F1-score\",size=param_dict['x_label_font'])\n",
    "    ax.set_title(f\"Number of Train Apps: {int(num_known_train_apps)} \\n Total: {temp_total}\",size=param_dict['title_size'])\n",
    "\n",
    "    plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "    plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "\n",
    "    plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6ae68",
   "metadata": {},
   "source": [
    "## Plot Average Scores for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_f1_scores = result_df.groupby(['num_known_train_apps']).min()['macro_f1_score'].values\n",
    "print(\"Min F1 Scores \\n\", min_f1_scores)\n",
    "\n",
    "min_f1_scores_train_apps = result_df.groupby(['num_known_train_apps']).min()['train_apps'].values\n",
    "print(\"Min F1 Scores Train Apps \\n\", min_f1_scores_train_apps)\n",
    "min_f1_scores_test_apps = result_df.groupby(['num_known_train_apps']).min()['test_apps'].values\n",
    "print(\"Min F1 Scores Test Apps \\n\", min_f1_scores_test_apps)\n",
    "\n",
    "\n",
    "max_f1_scores = result_df.groupby(['num_known_train_apps']).max()['macro_f1_score'].values\n",
    "print(\"Max F1 Scores \\n\", max_f1_scores)\n",
    "\n",
    "max_f1_scores_train_apps = result_df.groupby(['num_known_train_apps']).max()['train_apps'].values\n",
    "print(\"Max F1 Scores Train Apps \\n\", max_f1_scores_train_apps)\n",
    "max_f1_scores_test_apps = result_df.groupby(['num_known_train_apps']).max()['test_apps'].values\n",
    "print(\"Max F1 Scores Test Apps \\n\", max_f1_scores_test_apps)\n",
    "\n",
    "\n",
    "max_f1_scores_apps = result_df.groupby(['num_known_train_apps']).max()['train_apps'].values\n",
    "print(\"Min F1 Scores Apps \\n\", max_f1_scores_apps)\n",
    "\n",
    "\n",
    "std_f1_scores = result_df.groupby(['num_known_train_apps']).std()['macro_f1_score'].values\n",
    "print(\"Std F1 Scores \\n\", std_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe206c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_rate_df = result_rate_df.groupby(['num_known_train_apps','score_name']).max()\n",
    "far_df = grouped_rate_df.loc[(slice(None), 'false_alarm_rate'), :]\n",
    "amr_df = grouped_rate_df.loc[(slice(None), 'anom_miss_rate'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "far_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fe8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "                'fig_width': 28,\n",
    "                'fig_height': 12,\n",
    "                'y_label_font': 42,\n",
    "                'x_label_font': 42,\n",
    "                'x_ticks_font': 32,\n",
    "                'y_ticks_font': 36,    \n",
    "                'legend_size': 30,\n",
    "                'legend_title_size': 40,\n",
    "                'title_size': 45,\n",
    "                'title_pad': 40,\n",
    "                'suptitle_font': 25,\n",
    "                'suptitle_y': 1.1,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437271ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "\n",
    "sns.barplot(\n",
    "            x=\"num_known_train_apps\", \n",
    "            y=\"macro_f1_score\", \n",
    "            ax=axs[0],\n",
    "            palette=sns.color_palette(\"hls\", 8)[:4],\n",
    "            data=result_df) \n",
    "\n",
    "#fig.suptitle(f\"{int(result_df['num_unknown_test_apps'].unique()[0])} Unknowns in the Test Set\",size=param_dict['suptitle_font'], y=param_dict['suptitle_y'])\n",
    "#fig.suptitle(f\"Supervised Models\",size=param_dict['suptitle_font'], y=param_dict['suptitle_y'])\n",
    "fig.suptitle(\"\")\n",
    "\n",
    "axs[0].set_ylabel('F1-Score (Macro Avg)', size=param_dict['y_label_font'])\n",
    "axs[0].set_xlabel(f'Number of Training Apps',size=param_dict['x_label_font'])\n",
    "axs[0].set_title(f\"\",size=param_dict['title_size'])\n",
    "\n",
    "axs[0].tick_params(axis='x', labelsize= param_dict['x_ticks_font'])\n",
    "axs[0].tick_params(axis='y', labelsize= param_dict['y_ticks_font'])\n",
    "\n",
    "axs[0].axhline(y=0.99, linewidth=4, color='m', linestyle='--', label='5-CV')                 \n",
    "axs[0].set_ylim(0.65,1.01)\n",
    "axs[1].set_ylim(-0.01,0.4)\n",
    "\n",
    "                 \n",
    "sns.barplot(\n",
    "            x=\"num_known_train_apps\", \n",
    "            y=\"score\", \n",
    "            hue=\"score_name\",\n",
    "            ax=axs[1],\n",
    "            palette=sns.color_palette(\"hls\", 8)[4:6],\n",
    "            data=result_rate_df) \n",
    "                 \n",
    "                 \n",
    "axs[1].set_ylabel('', size=param_dict['y_label_font'])\n",
    "axs[1].set_xlabel(f'Number of Training Apps',size=param_dict['x_label_font'])\n",
    "axs[1].set_title(f\"\",size=param_dict['title_size'])\n",
    "\n",
    "axs[1].tick_params(axis='x', labelsize= param_dict['x_ticks_font'])\n",
    "axs[1].tick_params(axis='y', labelsize= param_dict['y_ticks_font']) \n",
    "\n",
    "handles_0, labels_0 = axs[0].get_legend_handles_labels()           \n",
    "\n",
    "\n",
    "legend=fig.legend(\n",
    "            handles_0,\n",
    "            ['F1-score$_{CV}$',\"False Alarm Rate\",\"Anomaly Miss Rate\",\"$FAR_{CV}$\",\"$AMR_{CV}$\"],\n",
    "            bbox_to_anchor=(0.13, 0.62, 0.25, 0.45),\n",
    "            #loc='best', \n",
    "            ncol = 1, \n",
    "            title = \"\",\n",
    "            frameon = False,\n",
    "            prop={'size': param_dict['legend_size']}, \n",
    "          )\n",
    "\n",
    "axs[1].axhline(y=0.008, linewidth=4, color='k', linestyle='--', label=\"$FAR_{CV}$\")  \n",
    "axs[1].axhline(y=0.022, linewidth=4, color='r', linestyle='--', label=\"$AMR_{CV}$\")                 \n",
    "\n",
    "handles, labels = axs[1].get_legend_handles_labels()           \n",
    "                          \n",
    "legend=fig.legend(\n",
    "            handles,\n",
    "            [\"$FAR_{CV}$\",\"$AMR_{CV}$\",\"False Alarm Rate (FAR)\",\"Anomaly Miss Rate (AMR)\"],\n",
    "            bbox_to_anchor=(0.50, 0.65, 0.45, 0.45),\n",
    "           #loc='best', \n",
    "            ncol = 2, \n",
    "            title = \"\",\n",
    "            frameon = False,    \n",
    "            prop={'size': param_dict['legend_size']}, \n",
    "          )\n",
    "\n",
    "axs[1].get_legend().remove()\n",
    "\n",
    "             \n",
    "plt.tight_layout() \n",
    "plt.savefig(f\"/usr3/graduate/baksar/projectx/AI4HPCAnalytics/src/unseen_experiments/plots/{SYSTEM}_unseen_motivational.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5afbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5b624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6db96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3ae98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96c3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35647f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6e243c3",
   "metadata": {},
   "source": [
    "## Plot All Unique App Combinations in the Test Set for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add85c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "                'fig_width': 18,\n",
    "                'fig_height': 12,\n",
    "                'y_label_font': 45,\n",
    "                'x_label_font': 45,\n",
    "                 'x_ticks_font': 42,\n",
    "                 'y_ticks_font': 50,    \n",
    "                'legend_size': 26,\n",
    "                'legend_title_size': 55,\n",
    "                'title_size': 36,\n",
    "                'title_pad': 40,             \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaebb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for app_combo in unique_test_app_combos: \n",
    "    \n",
    "    temp_result_df = result_df[result_df['test_apps'] == app_combo]\n",
    "    curr_train_apps = \"-\".join(set(all_apps) - set(app_combo.split('-')))\n",
    "        \n",
    "    fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "    ax = sns.barplot(x=\"num_known_train_apps\", \n",
    "                     y=\"macro_f1_score\", \n",
    "                     data=temp_result_df) \n",
    "        \n",
    "    ax.set_ylabel('Macro F1 Score',size=param_dict['y_label_font'])\n",
    "    ax.set_xlabel(f'Number of unknowns',size=param_dict['x_label_font'])\n",
    "    ax.set_title(f\"Train Apps: {curr_train_apps} \\n Test Apps: {app_combo}\",size=param_dict['title_size'])\n",
    "\n",
    "    plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "    plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "\n",
    "    plt.axhline(y=0.99,linewidth=4, color='r', linestyle='--', label='5-CV')\n",
    "\n",
    "    ax.set(ylim=(0.4, 1))\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0463e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47846d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43aae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a66a4bcb",
   "metadata": {},
   "source": [
    "## Plot Macro Average F1-scores According to Selected App Combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "                'fig_width': 18,\n",
    "                'fig_height': 12,\n",
    "                'y_label_font': 45,\n",
    "                'x_label_font': 45,\n",
    "                 'x_ticks_font': 42,\n",
    "                 'y_ticks_font': 50,    \n",
    "                'legend_size': 26,\n",
    "                'legend_title_size': 55,\n",
    "                'title_size': 36,\n",
    "                'title_pad': 40,             \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ca937",
   "metadata": {},
   "outputs": [],
   "source": [
    "for app_combo in app_combos: \n",
    "    temp_result_df = result_df[result_df['train_apps'] == app_combo]\n",
    "    curr_test_apps = \"-\".join(set(all_test_apps) - set(app_combo.split('-')))\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "    ax = sns.barplot(x=\"num_unknowns\", \n",
    "                     y=\"macro_f1_score\", \n",
    "                     data=temp_result_df) \n",
    "        \n",
    "    ax.set_ylabel('Macro F1 Score',size=param_dict['y_label_font'])\n",
    "    ax.set_xlabel(f'Number of unknowns',size=param_dict['x_label_font'])\n",
    "    ax.set_title(f\"Train Apps: {app_combo} \\n Test Apps: {curr_test_apps}\",size=param_dict['title_size'])\n",
    "\n",
    "    plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "    plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "\n",
    "    plt.axhline(y=0.99,linewidth=4, color='r', linestyle='--', label='5-CV')\n",
    "\n",
    "    ax.set(ylim=(0.4, 1))\n",
    "\n",
    "    plt.tight_layout()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2adcd7a",
   "metadata": {},
   "source": [
    "## Plot Anomaly-wise F1-scores According to Selected App Combos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_score = 'precision' # f1-score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43762c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "                'fig_width': 24,\n",
    "                'fig_height': 16,\n",
    "                'y_label_font': 45,\n",
    "                'x_label_font': 45,\n",
    "                 'x_ticks_font': 42,\n",
    "                 'y_ticks_font': 50,    \n",
    "                'legend_size': 26,\n",
    "                'legend_title_size': 35,\n",
    "                'title_size': 36,\n",
    "                'title_pad': 40,             \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af42474",
   "metadata": {},
   "outputs": [],
   "source": [
    "for app_combo in app_combos: \n",
    "    temp_result_df = result_anom_df[(result_anom_df['train_apps'] == app_combo) & (result_anom_df['score_name'] == selected_score)]\n",
    "    curr_test_apps = \"-\".join(set(all_test_apps) - set(app_combo.split('-')))\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(param_dict['fig_width'], param_dict['fig_height']))\n",
    "    ax = sns.barplot(x=\"num_unknowns\", \n",
    "                     y=\"score\", \n",
    "                     hue=\"anom_name\",\n",
    "                     data=temp_result_df) \n",
    "        \n",
    "    ax.set_ylabel(f'{selected_score}',size=param_dict['y_label_font'])\n",
    "    ax.set_xlabel(f'Number of unknowns',size=param_dict['x_label_font'])\n",
    "    ax.set_title(f\"Train Apps: {app_combo} \\n Test Apps: {curr_test_apps}\",size=param_dict['title_size'])\n",
    "\n",
    "    plt.xticks(fontsize=param_dict['x_ticks_font'])\n",
    "    plt.yticks(fontsize=param_dict['y_ticks_font'])\n",
    "\n",
    "    plt.axhline(y=0.99,linewidth=4, color='r', linestyle='--', label='5-CV')\n",
    "\n",
    "    ax.set(ylim=(0, 1))\n",
    "    \n",
    "    legend=plt.legend(bbox_to_anchor=(0.4, 0.8, 0.45, 0.45),\n",
    "               #loc='upper center', \n",
    "               ncol = 6, \n",
    "               title = \"Anomalies\",\n",
    "               prop={'size': param_dict['legend_size']}, \n",
    "              )\n",
    "    plt.setp(legend.get_title(),fontsize=param_dict['legend_title_size'])    \n",
    "\n",
    "    plt.tight_layout()      \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_result_df[(temp_result_df['anom_name'] == 'none') & (temp_result_df['score_name'] == 'precision')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_anom_df[(result_anom_df['anom_name']=='dial') & (result_anom_df['score_name']=='f1-score') & (result_anom_df['num_unknowns']==4)].sort_values(by=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9929627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5852cac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c42c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
