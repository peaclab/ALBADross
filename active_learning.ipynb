{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575aa52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44028fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-7s %(message)s',\n",
    "                    stream=sys.stderr, level=logging.INFO)\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.INFO)\n",
    "\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "\n",
    "#General ML \n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, silhouette_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from modules.clustering_helpers import select_labeled_samples\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Active Learning\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling, margin_sampling, entropy_sampling\n",
    "\n",
    "\n",
    "#In-house Module Imports\n",
    "from config import Configuration \n",
    "from datasets import EclipseSampledDataset, VoltaSampledDataset\n",
    "from utils import *\n",
    "\n",
    "def random_sampling(classifier, X_pool):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples))\n",
    "    return query_idx, X_pool[query_idx]\n",
    "\n",
    "def equal_app_sampling(classifier, X_pool, app_labels):\n",
    "    \"\"\"\n",
    "        Selects one random sample from each application exist in the app_labels.Assumes the given \n",
    "        app_labels has all the applications \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    query_idx = []\n",
    "    for app in np.unique(app_labels): \n",
    "        query_idx.append(np.random.choice(np.where(app_labels == app)[0],1)[0])\n",
    "        \n",
    "    query_idx = np.array(query_idx)\n",
    "    return query_idx, X_pool[query_idx]\n",
    "\n",
    "\n",
    "def call_FAR_function(false_alarm_rates,anomaly_miss_rates, test_label, y_pred, conf):\n",
    "    false_alarm_rate, anom_miss_rate = FAR_AMR_Calculate(\n",
    "            true_label= test_label['anom'].to_numpy(),\n",
    "            pred_label= y_pred,\n",
    "            result_dir= str(conf['results_dir']),\n",
    "            save_name= \"\",\n",
    "            save=False,\n",
    "            verbose=False,\n",
    "    )\n",
    "    false_alarm_rates.append(false_alarm_rate)\n",
    "    anomaly_miss_rates.append(anom_miss_rate)\n",
    "\n",
    "query_strategy_dict = {\n",
    "                       \"uncertainty\": uncertainty_sampling, \n",
    "                       \"margin\": margin_sampling, \n",
    "                       \"entropy\": entropy_sampling,\n",
    "                       \"random\": random_sampling,\n",
    "                        \"equal_app\": equal_app_sampling\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = \"app_sampling_implementation\"  # change this\n",
    "SYSTEM = 'volta'  # volta or eclipse\n",
    "FE_NAME = 'tsfresh' #tsfresh, or mvts => It will set the EXP_NAME. Be careful. \n",
    "NUM_FEATURE = 2000  # example: 250 ,2000, 4000\n",
    "classifier_name = 'rf'\n",
    "query_strategy = \"equal_app\"  # \"uncertainty\", \"margin\", \"entropy\", \"random\", \"equal_app\"\n",
    "CV_INDEX = 0  # it can be integer value within the range 0 1 2 3 4\n",
    "repeat_num = 0\n",
    "query_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f54b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "FS_NAME = \"CHI\"\n",
    "method = \"baseline\" if (query_strategy == 'random' or query_strategy == 'equal_app')  else \"active_learning\"\n",
    "num_samples_per_pair = 1\n",
    "if SYSTEM == 'volta':\n",
    "    OUTPUT_DIR = f'/projectnb/peaclab-mon/{user}/active_learning_experiments'\n",
    "elif SYSTEM == 'eclipse':    \n",
    "    OUTPUT_DIR = f'/projectnb/peaclab-mon/{user}/active_learning_experiments_final_hdfs'\n",
    "EXP_NAME = f'{FE_NAME}_experiments'\n",
    "FEATURE_SELECTION = False\n",
    "SCALER = 'None' #For now, do the scaling inside the notebook, then you can move that to the class function\n",
    "\n",
    "logging.warning('Results will be generated in {}, double check please!'.format(MODEL_CONFIG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7400f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Configuration(ipython=True,\n",
    "                     overrides={\n",
    "                         'output_dir': Path(OUTPUT_DIR), #change\n",
    "                         'system' : SYSTEM,\n",
    "                         'exp_name':EXP_NAME,                                                  \n",
    "                         'cv_fold':CV_INDEX, \n",
    "                         'model_config': MODEL_CONFIG,\n",
    "                     })\n",
    "\n",
    "with open(str(conf['experiment_dir']) + '/anom_dict.json') as f:\n",
    "    ANOM_DICT = json.load(f)\n",
    "with open(str(conf['experiment_dir']) + '/app_dict.json') as f:\n",
    "    APP_DICT = json.load(f) \n",
    "    \n",
    "APP_REVERSE_DICT = {}\n",
    "for app_name, app_encoding in APP_DICT.items():\n",
    "    APP_REVERSE_DICT[app_encoding] = app_name    \n",
    "\n",
    "ANOM_REVERSE_DICT = {}\n",
    "for anom_name, anom_encoding in ANOM_DICT.items():\n",
    "    ANOM_REVERSE_DICT[anom_encoding] = anom_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c127215",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SYSTEM == 'eclipse':\n",
    "    eclipseDataset = EclipseSampledDataset(conf)\n",
    "    train_data, train_label, test_data, test_label = eclipseDataset.load_dataset(scaler=SCALER,\n",
    "                                                                                 cv_fold=CV_INDEX,\n",
    "                                                                                 borghesi=False, \n",
    "                                                                                 mvts=True if FE_NAME == 'mvts' else False, \n",
    "                                                                                 tsfresh=True if FE_NAME == 'tsfresh' else False)\n",
    "\n",
    "elif SYSTEM == 'volta':\n",
    "    voltaDataset = VoltaSampledDataset(conf)\n",
    "    train_data, train_label, test_data, test_label = voltaDataset.load_dataset(scaler=SCALER,\n",
    "                                                                               cv_fold=CV_INDEX,\n",
    "                                                                               borghesi=False,\n",
    "                                                                               mvts=True if FE_NAME == 'mvts' else False,\n",
    "                                                                               tsfresh=True if FE_NAME == 'tsfresh' else False)\n",
    "\n",
    "assert list(train_data.index) == list(train_label.index) #check the order of the labels     \n",
    "assert list(test_data.index) == list(test_label.index) #check the order of the labels    \n",
    "\n",
    "if FEATURE_SELECTION:\n",
    "    selected_features = pd.read_csv(conf['experiment_dir'] / 'selected_features.csv')\n",
    "    train_data = train_data[list(selected_features['0'].values)]\n",
    "    test_data = test_data[list(selected_features['0'].values)]\n",
    "    \n",
    "train_label['anom_names'] = train_label.apply(lambda x: ANOM_REVERSE_DICT[x['anom']], axis=1)\n",
    "train_label['app_names']=train_label['app'].apply(lambda x: APP_REVERSE_DICT[x])\n",
    "test_label['anom_names'] = test_label.apply(lambda x: ANOM_REVERSE_DICT[x['anom']], axis=1)\n",
    "test_label['app_names']=test_label['app'].apply(lambda x: APP_REVERSE_DICT[x])\n",
    "\n",
    "all_data = pd.concat([train_data, test_data])\n",
    "all_data = all_data.dropna(axis=1, how='any')\n",
    "all_label = pd.concat([train_label,test_label])\n",
    "\n",
    "train_data = all_data.loc[train_label.index]\n",
    "test_data = all_data.loc[test_label.index]\n",
    "    \n",
    "logging.info(\"Train data shape %s\",train_data.shape)\n",
    "logging.info(\"Train label shape %s\",train_label.shape)\n",
    "logging.info(\"Test data shape %s\",test_data.shape)  \n",
    "logging.info(\"Test label shape %s\",test_label.shape)\n",
    "\n",
    "logging.info(\"Train data label dist: \\n%s\",train_label['anom'].value_counts())\n",
    "logging.info(\"Test data label dist: \\n%s\",test_label['anom'].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c70a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = 'MinMax'\n",
    "\n",
    "if SCALER == 'MinMax':\n",
    "    \n",
    "    minmax_scaler = MinMaxScaler().fit(train_data)\n",
    "    train_data = pd.DataFrame(minmax_scaler.transform(train_data),columns=train_data.columns,index=train_data.index)\n",
    "    test_data = pd.DataFrame(minmax_scaler.transform(test_data),columns=test_data.columns,index=test_data.index)\n",
    "    \n",
    "elif SCALER == 'Standard':\n",
    "    \n",
    "    # Standardize data (per feature Z-normalization, i.e. zero-mean and unit variance)        \n",
    "    scaler = StandardScaler().fit(train_data)\n",
    "    train_data = pd.DataFrame(scaler.transform(train_data),columns=train_data.columns,index=train_data.index)\n",
    "    test_data = pd.DataFrame(scaler.transform(test_data),columns=test_data.columns,index=test_data.index)  \n",
    "    \n",
    "#Implement new feature selection strategies below\n",
    "if FS_NAME == 'CHI':\n",
    "    \n",
    "    selector = SelectKBest(chi2, k=NUM_FEATURE)\n",
    "    selector.fit(train_data,train_label['anom'])\n",
    "    train_data = train_data[train_data.columns[selector.get_support(indices=True)]]\n",
    "    selected_columns = train_data.columns\n",
    "    test_data = test_data[test_data.columns & selected_columns]\n",
    "    \n",
    "elif FS_NAME == 'TSFRESH':\n",
    "    logging.warning(\"NUM_FEATURE parameter will be overwritten by the automatic selection process\")\n",
    "    \n",
    "    y_train = train_label['anom']\n",
    "    X_train = train_data\n",
    "\n",
    "    relevant_features = set()\n",
    "\n",
    "    for label in y_train.unique():\n",
    "        y_train_binary = y_train == label\n",
    "        X_train_filtered = tsfresh.select_features(X_train, y_train_binary)\n",
    "        print(\"Number of relevant features for class {}: {}/{}\".format(label, X_train_filtered.shape[1], X_train.shape[1]))\n",
    "        relevant_features = relevant_features.union(set(X_train_filtered.columns))    \n",
    "    train_data = train_data[relevant_features]\n",
    "    test_data = test_data[relevant_features]\n",
    "    NUM_FEATURE = len(relevant_features)\n",
    "    \n",
    "elif FS_NAME == 'NONE':\n",
    "    logging.info(\"No feature selection strategy is specified, will be using all features\")\n",
    "    NUM_FEATURE = len(train_data.columns)\n",
    "    \n",
    "logging.info(train_data.shape)\n",
    "logging.info(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b819328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the node_ids considered labeled\n",
    "labeled_train_label = pd.read_csv(conf['experiment_dir'] / f'CV_{CV_INDEX}'/ f'labeled_train_label_{num_samples_per_pair}.csv', index_col=['node_id'])\n",
    "labeled_test_label = pd.read_csv(conf['experiment_dir'] / f'CV_{CV_INDEX}'/ f'labeled_test_label_{num_samples_per_pair}.csv', index_col=['node_id'])\n",
    "node_indices_labeled = list(labeled_train_label['anom'].index.values)\n",
    "\n",
    "logging.info(\"Labeled data label dist: \\n%s\",labeled_train_label['anom'].value_counts())\n",
    "logging.info(\"Unlabeled data label dist: \\n%s\",labeled_test_label['anom'].value_counts())\n",
    "\n",
    "#Set a new column for label status\n",
    "node_indices_unlabeled = []\n",
    "for node in train_label.index:\n",
    "    if node not in node_indices_labeled:\n",
    "        node_indices_unlabeled.append(node)\n",
    "train_label['label_status'] = train_label['anom'] # for the full data case\n",
    "train_label['label_status'] = np.where(train_label.index.get_level_values('node_id').isin(node_indices_unlabeled), -1,train_label['label_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab58e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_labeled_pool contains one sample from each application anomaly pair\n",
    "initial_labeled_pool = train_label[(train_label['label_status'] != -1)]\n",
    "#Active learning or random sampling will be querying from the same pool\n",
    "initial_unlabeled_pool = train_label[(train_label['label_status'] == -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0698198",
   "metadata": {},
   "outputs": [],
   "source": [
    "if classifier_name == 'rf':\n",
    "    best_params = {\n",
    "            \"criterion\": \"entropy\" if SYSTEM == \"eclipse\" else  \"entropy\",\n",
    "            \"max_depth\": 8 if SYSTEM == \"eclipse\" else 8,\n",
    "            \"n_estimators\": 200 if SYSTEM == \"eclipse\" else 20,\n",
    "    } \n",
    "    \n",
    "    selected_classifier = RandomForestClassifier(\n",
    "                                            criterion = best_params[\"criterion\"],\n",
    "                                            max_depth = best_params[\"max_depth\"],\n",
    "                                            n_estimators = best_params[\"n_estimators\"],                                                            \n",
    "                                            )\n",
    "    \n",
    "elif classifier_name == 'lr':\n",
    "    best_params = {\n",
    "            \"C\": 1.0 if SYSTEM == \"eclipse\" else 10,\n",
    "            \"penalty\": \"l1\" if SYSTEM == \"eclipse\" else  \"l1\",\n",
    "            \"solver\": \"liblinear\" if SYSTEM == \"eclipse\" else  \"liblinear\",\n",
    "    } \n",
    "    \n",
    "    selected_classifier = LogisticRegression(\n",
    "                                        C = best_params[\"C\"],\n",
    "                                        penalty = best_params[\"penalty\"],\n",
    "                                        solver = best_params[\"solver\"],        \n",
    "                                        )\n",
    "    \n",
    "elif classifier_name == 'mlp':\n",
    "    \n",
    "    best_params = {\n",
    "            \"alpha\": 0.0001 if SYSTEM == \"eclipse\" else 0.01,\n",
    "            \"hidden_layer_sizes\": [50, 100, 50] if SYSTEM == \"eclipse\" else [100],\n",
    "            \"max_iter\": 100 if SYSTEM == \"eclipse\" else 100,            \n",
    "    } \n",
    "    \n",
    "    selected_classifier = MLPClassifier(random_state=1, \n",
    "                                        alpha = best_params['alpha'],\n",
    "                                        hidden_layer_sizes = best_params['hidden_layer_sizes'],\n",
    "                                        max_iter = best_params['max_iter'],                                                                              \n",
    "                                       )\n",
    "elif classifier_name == 'lgbm': \n",
    "\n",
    "    best_params = {\n",
    "            \"num_leaves\": 31 if SYSTEM == \"eclipse\" else 128,\n",
    "            \"learning_rate\": 0.1 if SYSTEM == \"eclipse\" else 0.1,\n",
    "            \"max_depth\": -1 if SYSTEM == \"eclipse\" else 8,\n",
    "            \"colsample_bytree\": 1 if SYSTEM == \"eclipse\" else 1,\n",
    "    } \n",
    "    selected_classifier = LGBMClassifier(objective=\"multiclass\", \n",
    "                                         num_leaves = best_params['num_leaves'],\n",
    "                                         learning_rate = best_params['learning_rate'],\n",
    "                                         max_depth = best_params['max_depth'],\n",
    "                                         colsample_bytree = best_params['colsample_bytree'],                                         \n",
    "                                         random_state=5)\n",
    "    \n",
    "else:\n",
    "    selected_classifier = RandomForestClassifier()\n",
    "    \n",
    "logging.info(selected_classifier)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae46105",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()\n",
    "all_app_names = list(APP_DICT.keys())\n",
    "selected_apps= dict.fromkeys(all_app_names,0)\n",
    "selected_anoms= dict.fromkeys(list(ANOM_REVERSE_DICT.keys()),0)\n",
    "\n",
    "#Create the label and data for the starting condition composed of selected apps \n",
    "y_initial = initial_labeled_pool\n",
    "x_initial = train_data[train_data.index.get_level_values('node_id').isin(y_initial.index)]\n",
    "y_initial = y_initial['anom'].to_numpy()\n",
    "x_initial = x_initial.to_numpy()                \n",
    "\n",
    "x_unlabeled = train_data[train_data.index.get_level_values('node_id').isin(initial_unlabeled_pool.index)]\n",
    "y_unlabeled = initial_unlabeled_pool\n",
    "x_unlabeled = x_unlabeled.to_numpy()                \n",
    "\n",
    "#Initializations\n",
    "macro_f1_scores = []\n",
    "anomaly_miss_rates = []\n",
    "false_alarm_rates = []\n",
    "\n",
    "if query_strategy != \"random\":\n",
    "    selected_indices_apps = []\n",
    "    selected_indices_anoms = []\n",
    "    \n",
    "if query_strategy == 'equal_app':\n",
    "    step = len(all_app_names)\n",
    "else:\n",
    "    step = 1\n",
    "\n",
    "x_pool = x_unlabeled.copy()\n",
    "y_pool = y_unlabeled.copy() \n",
    "y_pool_anom = y_pool['anom'].to_numpy()\n",
    "y_pool_app = y_pool['app_names'].to_numpy()                \n",
    "\n",
    "learner = ActiveLearner(\n",
    "    estimator=selected_classifier,\n",
    "    query_strategy=query_strategy_dict[query_strategy],\n",
    "    X_training=x_initial, \n",
    "    y_training=y_initial\n",
    ")        \n",
    "\n",
    "y_pred = learner.predict(test_data.to_numpy())\n",
    "report_dict = classification_report(test_label['anom'].to_numpy(), y_pred, output_dict = True)\n",
    "macro_f1_scores.append(report_dict['macro avg']['f1-score'])                                        \n",
    "print(report_dict)\n",
    "call_FAR_function(false_alarm_rates, anomaly_miss_rates, test_label, y_pred, conf)\n",
    "\n",
    "for i in range(0,query_size,step):\n",
    "    \n",
    "    if not i % 50: \n",
    "        logging.info(f\"Current query index: {i}\")\n",
    "    \n",
    "    if query_strategy == 'equal_app':\n",
    "        query_idx, query_sample = learner.query(x_pool, app_labels=y_pool_app)\n",
    "        learner.teach(\n",
    "            X=x_pool[query_idx],\n",
    "            y=y_pool_anom[query_idx],\n",
    "        )\n",
    "        #logging.info(np.unique(query_idx,return_counts=True))\n",
    "        \n",
    "    else:\n",
    "        query_idx, query_sample = learner.query(x_pool)        \n",
    "        learner.teach(\n",
    "            X=x_pool[query_idx].reshape(1,-1),\n",
    "            y=y_pool_anom[query_idx].reshape(1,)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    if query_strategy != \"random\":\n",
    "        selected_indices_apps.append(y_pool_app[query_idx][0])\n",
    "        selected_indices_anoms.append(y_pool_anom[query_idx][0])\n",
    "\n",
    "\n",
    "    x_pool, y_pool_anom, y_pool_app = np.delete(x_pool, query_idx, axis=0), np.delete(y_pool_anom, query_idx, axis=0), np.delete(y_pool_app, query_idx, axis=0)\n",
    "    y_pred = learner.predict(test_data.to_numpy())              \n",
    "\n",
    "    report_dict = classification_report(test_label['anom'].to_numpy(), y_pred, output_dict = True)\n",
    "    macro_f1_scores.append(report_dict['macro avg']['f1-score'])                                        \n",
    "    call_FAR_function(false_alarm_rates,anomaly_miss_rates, test_label, y_pred, conf)    \n",
    "    \n",
    "\n",
    "for j in range(0,len(macro_f1_scores)):\n",
    "    scores = scores.append({'query_iter':j*step,\n",
    "                            'macro_avg_f1_score':macro_f1_scores[j],\n",
    "                            'false_alarm_rate':false_alarm_rates[j],\n",
    "                            'anomaly_miss_rate':anomaly_miss_rates[j], \n",
    "                            'repeat_num':repeat_num},\n",
    "                           ignore_index = True)\n",
    "\n",
    "scores['fold'] = CV_INDEX\n",
    "scores['method'] = method\n",
    "scores['query_strategy'] = query_strategy\n",
    "scores['model'] = selected_classifier.__class__.__name__\n",
    "scores['dataset'] = SYSTEM\n",
    "scores['fe'] = FE_NAME\n",
    "scores['feature_count'] = NUM_FEATURE   \n",
    "scores['query_size'] = query_size\n",
    "\n",
    "scores = scores.sort_values(by = ['query_iter']).reset_index(drop = True)\n",
    "\n",
    "train_app_names = \"all\"\n",
    "test_app_names = \"all\"\n",
    "\n",
    "filename = f\"train:{train_app_names}#test:{test_app_names}#{FE_NAME}#{NUM_FEATURE}#{method}#{query_strategy}#{query_size}#{selected_classifier.__class__.__name__}#{repeat_num}.csv\"\n",
    "scores.to_csv(Path(conf[\"results_dir\"]) / filename)\n",
    "\n",
    "logging.info(\"Saving: %s\", filename)\n",
    "\n",
    "if query_strategy != \"random\":\n",
    "    selected_app_anom_df = pd.DataFrame()\n",
    "    selected_app_anom_df['apps'] = selected_indices_apps\n",
    "    selected_app_anom_df['anoms'] = selected_indices_anoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f43f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
